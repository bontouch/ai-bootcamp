{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 Thursday Session 1: Retrieval & Context Systems (RAG)\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the limitations of LLMs without external context\n",
    "- Learn how to inject domain-specific knowledge using context\n",
    "- Explore basic RAG patterns with embeddings and vector search\n",
    "- Practice chunking strategies and retrieval techniques\n",
    "\n",
    "## Setup\n",
    "Make sure you have your OpenAI API key set as an environment variable or in a `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from typing import List, Dict\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 1: Testing LLM Knowledge Limits\n",
    "\n",
    "Let's start by asking the model something it won't know - questions about Framna, our company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What services does Framna offer and what technology stack do they use?\n",
      "Answer: As of now, there is **no widely recognized company or product known as \"Framna\"** in major business, software, or technology sectors (as of June 2024). It’s possible that:\n",
      "\n",
      "- **You may be referring to a lesser-known local business or startup.**\n",
      "- **There may be a typo in the name** (for example, “Frama,” “Framnair,” or “Framna?”).\n",
      "- **It could be a new company with limited online presence.**\n",
      "\n",
      "### What to Do Next\n",
      "If you have **additional context** (like their website, location, industry, or a link), please provide it—I can help investigate further!\n",
      "\n",
      "---\n",
      "\n",
      "#### If you meant a different company (e.g., “Frama”) here’s a quick mention:\n",
      "\n",
      "- **Frama** (https://frama.com/) is a Danish design company offering home, furniture, and lighting products—not really a tech/software company.\n",
      "- **Frama Communications** offers Swiss-based\n"
     ]
    }
   ],
   "source": [
    "def ask_question(question: str) -> str:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4.1\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ],\n",
    "        max_tokens=200\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Try asking about Framna\n",
    "question = \"What services does Framna offer and what technology stack do they use?\"\n",
    "answer = ask_question(question)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Result:** The model won't know about Framna since it's not in its training data.\n",
    "\n",
    "## Challenge 2: Adding Context - The Naive Approach\n",
    "\n",
    "Now let's load our company information and inject it as context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7771 characters of company information\n",
      "First 200 characters: # Framna\n",
      "\n",
      "## About\n",
      "\n",
      "Framna was born from the strong product-led culture of digital agencies Bontouch, Move, and Shape. Today, we are more than 500 specialists across eight studios in Denmark, Sweden, ...\n"
     ]
    }
   ],
   "source": [
    "# Load company information\n",
    "with open('framna_company_info.txt', 'r', encoding='utf-8') as f:\n",
    "    company_info = f.read()\n",
    "\n",
    "print(f\"Loaded {len(company_info)} characters of company information\")\n",
    "print(f\"First 200 characters: {company_info[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What services does Framna offer and what technology stack do they use?\n",
      "Answer with context: **Services Offered by Framna**\n",
      "\n",
      "Framna offers comprehensive digital product services spanning the entire product lifecycle. Specifically, they:\n",
      "\n",
      "1. **Explore**\n",
      "   - Identify strategic opportunities\n",
      "   - Analyze market trends and future scenarios\n",
      "   - Advise on emerging technologies\n",
      "\n",
      "2. **Create & Craft**\n",
      "   - Design digital products with a deep focus on UX and usability\n",
      "   - Build and launch high-performance digital products, particularly native mobile apps for iOS and Android\n",
      "\n",
      "3. **Lead & Grow**\n",
      "   - Scale products to market-leading positions\n",
      "   - Drive growth and ongoing product optimization\n",
      "   - Support achieving and sustaining product-market fit\n",
      "\n",
      "4. **Sustain**\n",
      "   - Ensure stability, reliability, and continuous improvement for products\n",
      "   - Maintain and optimize products in-market for long-term success\n",
      "\n",
      "**Additional Offerings:**\n",
      "- Embedded dedicated product teams within client organizations for deep collaboration\n",
      "- Consulting and advisory in product strategy and technology trends\n",
      "- Research, whitepapers, thought leadership, and industry reports (e.g., Mobile App Trends Report)\n",
      "- Event organization (e.g., Future Product Days)\n",
      "\n",
      "**Industries Served:**\n",
      "- Rail/transport, airport/aviation, energy/utilities, health/medical, retail/e-commerce, banking/finance, and more\n",
      "\n",
      "---\n",
      "\n",
      "**Technology Stack**\n",
      "\n",
      "Based on the provided information, Framna:\n",
      "\n",
      "- **Specializes in native mobile app development:**  \n",
      "  - **Platforms:** iOS (Apple), Android (Google)\n",
      "\n",
      "- **Emphasizes\n"
     ]
    }
   ],
   "source": [
    "def ask_with_context(question: str, context: str) -> str:\n",
    "    \"\"\"Ask a question with provided context using string formatting\"\"\"\n",
    "    \n",
    "    # Define our prompt template with placeholders\n",
    "    prompt_template = \"\"\"Based on the following company information, please answer the question.\n",
    "\n",
    "Company Information:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer based only on the provided information:\"\"\"\n",
    "    \n",
    "    formatted_prompt = prompt_template.format(\n",
    "        context=context,\n",
    "        question=question\n",
    "    )\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4.1\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": formatted_prompt}\n",
    "        ],\n",
    "        max_tokens=300\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Now ask the same question with context\n",
    "answer_with_context = ask_with_context(question, company_info)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer with context: {answer_with_context}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Result:** Now the model can answer accurately about Framna!\n",
    "\n",
    "Let's try a few more questions to see how well this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test more questions\n",
    "questions = [\n",
    "    \"Where is Framna located and what is their work policy?\",\n",
    "    \"What are Framna's company values?\",\n",
    "    \"What notable projects has Framna worked on?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    answer = ask_with_context(q, company_info)\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    print(f\"A: {answer}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 3: The Problem with Large Documents\n",
    "\n",
    "What happens when we have too much information? Let's simulate a larger document and see the limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original document: 7771 characters\n",
      "Large document: 77710 characters\n",
      "Estimated tokens: 111014.0\n",
      "\n",
      "⚠️ Problems with large documents:\n",
      "- Higher API costs (more tokens)\n",
      "- Slower response times\n",
      "- Context window limits (models have max token limits)\n",
      "- Difficulty finding relevant information (needle in haystack)\n"
     ]
    }
   ],
   "source": [
    "# Simulate a much larger document by repeating our content\n",
    "large_document = company_info * 10  # 10x the original size\n",
    "\n",
    "print(f\"Original document: {len(company_info)} characters\")\n",
    "print(f\"Large document: {len(large_document)} characters\")\n",
    "\n",
    "# Calculate approximate token count (rough estimate: 1 token ≈ 4 characters)\n",
    "estimated_tokens = len(large_document) // 0.7\n",
    "print(f\"Estimated tokens: {estimated_tokens}\")\n",
    "\n",
    "# This would be expensive and inefficient!\n",
    "print(\"\\n⚠️ Problems with large documents:\")\n",
    "print(\"- Higher API costs (more tokens)\")\n",
    "print(\"- Slower response times\")\n",
    "print(\"- Context window limits (models have max token limits)\")\n",
    "print(\"- Difficulty finding relevant information (needle in haystack)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 4: Introduction to RAG - Q&A Database Approach\n",
    "\n",
    "Now let's see a more practical example. We have a large Q&A database about Framna. It would be inefficient to send all Q&A pairs for every question. Instead, we'll use RAG to find only the relevant Q&A pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromaDB collection created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Install ChromaDB if not already installed\n",
    "# !pip install chromadb\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "chroma_client = chromadb.Client(Settings(anonymized_telemetry=False))\n",
    "\n",
    "# Create a collection\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"framna_knowledge\",\n",
    "    metadata={\"description\": \"Framna company information\"}\n",
    ")\n",
    "\n",
    "print(\"ChromaDB collection created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Q&A database with 3306 characters\n",
      "Parsed 20 Q&A pairs\n",
      "\n",
      "First few Q&A pairs:\n",
      "\n",
      "--- Q&A 1 ---\n",
      "Q: What services does Framna offer?\n",
      "A: Framna offers digital product strategy and design, frontend and backend development, mobile app development (iOS and Android), cloud infrastructure and DevOps, data analytics and AI integration, and product management consulting.\n",
      "\n",
      "--- Q&A 2 ---\n",
      "Q: What is Framna's main technology stack?\n",
      "A: Frontend: React, Next.js, TypeScript, Tailwind CSS. Backend: Node.js, Python, FastAPI, GraphQL. Mobile: React Native, Swift, Kotlin. Cloud: AWS, Azure, Docker, Kubernetes. Databases: PostgreSQL, MongoDB, Redis. AI/ML: OpenAI API, LangChain, vector databases.\n",
      "\n",
      "--- Q&A 3 ---\n",
      "Q: Where is Framna located?\n",
      "A: Framna is based in Stockholm, Sweden with a remote-first work policy allowing employees to work from anywhere in Europe.\n"
     ]
    }
   ],
   "source": [
    "# Load our Q&A database instead\n",
    "with open('framna_qa_database.txt', 'r', encoding='utf-8') as f:\n",
    "    qa_database = f.read()\n",
    "\n",
    "print(f\"Loaded Q&A database with {len(qa_database)} characters\")\n",
    "\n",
    "# Let's see what we're working with\n",
    "qa_pairs = []\n",
    "lines = qa_database.strip().split('\\n')\n",
    "current_q = None\n",
    "current_a = None\n",
    "\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if line.startswith('Q: '):\n",
    "        current_q = line[3:]  # Remove 'Q: '\n",
    "    elif line.startswith('A: '):\n",
    "        current_a = line[3:]  # Remove 'A: '\n",
    "        if current_q and current_a:\n",
    "            qa_pairs.append(f\"Q: {current_q}\\nA: {current_a}\")\n",
    "            current_q = None\n",
    "            current_a = None\n",
    "\n",
    "print(f\"Parsed {len(qa_pairs)} Q&A pairs\")\n",
    "print(f\"\\nFirst few Q&A pairs:\")\n",
    "for i, qa in enumerate(qa_pairs[:3]):\n",
    "    print(f\"\\n--- Q&A {i+1} ---\")\n",
    "    print(qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings for 20 Q&A pairs\n",
      "Each embedding has 1536 dimensions\n"
     ]
    }
   ],
   "source": [
    "def get_embeddings(texts: List[str]) -> List[List[float]]:\n",
    "    \"\"\"Get embeddings for a list of texts using OpenAI\"\"\"\n",
    "    response = client.embeddings.create(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        input=texts\n",
    "    )\n",
    "    return [embedding.embedding for embedding in response.data]\n",
    "\n",
    "# Get embeddings for our Q&A pairs (not chunks this time)\n",
    "embeddings = get_embeddings(qa_pairs)\n",
    "print(f\"Generated embeddings for {len(embeddings)} Q&A pairs\")\n",
    "print(f\"Each embedding has {len(embeddings[0])} dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 20 Q&A pairs to ChromaDB collection\n",
      "\n",
      "💡 Key insight: We have 20 Q&A pairs, but we only need 2-3 relevant ones to answer most questions!\n",
      "This saves tokens, reduces costs, and improves response quality.\n"
     ]
    }
   ],
   "source": [
    "# Add Q&A pairs to ChromaDB with embeddings\n",
    "collection.add(\n",
    "    documents=qa_pairs,\n",
    "    embeddings=embeddings,\n",
    "    ids=[f\"qa_{i}\" for i in range(len(qa_pairs))],\n",
    "    metadatas=[{\"qa_index\": i} for i in range(len(qa_pairs))]\n",
    ")\n",
    "\n",
    "print(f\"Added {len(qa_pairs)} Q&A pairs to ChromaDB collection\")\n",
    "\n",
    "# Demonstrate the concept: we don't need all 20 Q&A pairs to answer one question\n",
    "print(f\"\\n💡 Key insight: We have {len(qa_pairs)} Q&A pairs, but we only need 2-3 relevant ones to answer most questions!\")\n",
    "print(\"This saves tokens, reduces costs, and improves response quality.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 5: Semantic Search Experiments\n",
    "\n",
    "Now let's experiment with semantic search to find relevant chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Query: What programming languages does Framna use?\n",
      "\n",
      "📋 Most relevant Q&A 1 (distance: 0.528):\n",
      "Q: What backend technologies does Framna use?\n",
      "A: Framna uses Node.js, Python, FastAPI, and GraphQL for backend development.\n",
      "\n",
      "📋 Most relevant Q&A 2 (distance: 0.587):\n",
      "Q: What is Framna's main technology stack?\n",
      "A: Frontend: React, Next.js, TypeScript, Tailwind CSS. Backend: Node.js, Python, FastAPI, GraphQL. Mobile: React Native, Swift, Kotlin. Cloud: AWS, Azure, Docker, Kubernetes. Databases: PostgreSQL, MongoDB, Redis. AI/ML: OpenAI API, LangChain, vector databases.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🔍 Query: Company work environment and culture\n",
      "\n",
      "📋 Most relevant Q&A 1 (distance: 1.004):\n",
      "Q: What is Framna's company culture like?\n",
      "A: The company culture emphasizes continuous learning, innovation, and collaborative problem-solving.\n",
      "\n",
      "📋 Most relevant Q&A 2 (distance: 1.269):\n",
      "Q: What are Framna's company values?\n",
      "A: 1. Innovation through technology, 2. Sustainable and ethical business practices, 3. Collaborative teamwork, 4. Continuous learning and development, 5. Customer-centric approach.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🔍 Query: Geographic location and office setup\n",
      "\n",
      "📋 Most relevant Q&A 1 (distance: 1.350):\n",
      "Q: Where is Framna located?\n",
      "A: Framna is based in Stockholm, Sweden with a remote-first work policy allowing employees to work from anywhere in Europe.\n",
      "\n",
      "📋 Most relevant Q&A 2 (distance: 1.362):\n",
      "Q: What is Framna's remote work policy?\n",
      "A: Framna has a remote-first work policy allowing employees to work from anywhere in Europe.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def search_knowledge(query: str, n_results: int = 3) -> Dict:\n",
    "    \"\"\"Search for relevant Q&A pairs using semantic similarity\"\"\"\n",
    "    # Get embedding for the query\n",
    "    query_embedding = get_embeddings([query])[0]\n",
    "    \n",
    "    # Search in ChromaDB\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=n_results\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test semantic search with our Q&A database\n",
    "test_queries = [\n",
    "    \"What programming languages does Framna use?\",\n",
    "    \"Company work environment and culture\",  \n",
    "    \"Geographic location and office setup\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n🔍 Query: {query}\")\n",
    "    results = search_knowledge(query, n_results=2)\n",
    "    \n",
    "    for i, (doc, distance) in enumerate(zip(results['documents'][0], results['distances'][0])):\n",
    "        print(f\"\\n📋 Most relevant Q&A {i+1} (distance: {distance:.3f}):\")\n",
    "        print(f\"{doc}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 6: RAG System - Putting It All Together\n",
    "\n",
    "Now let's build a complete RAG system that retrieves relevant chunks and uses them as context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "❓ Question: What technology stack does Framna use for mobile development?\n",
      "\n",
      "💡 Answer: Framna uses React Native, Swift, and Kotlin for mobile development.\n",
      "\n",
      "📚 Q&A pairs used:\n",
      "\n",
      "  1. Q: What mobile development technologies does Framna use?\n",
      "A: Framna develops mobile apps using React Native, Swift, and Kotlin.\n",
      "\n",
      "  2. Q: What is Framna's main technology stack?\n",
      "A: Frontend: React, Next.js, TypeScript, Tailwind CSS. Backend: Node.js, Python, FastAPI, GraphQL. Mobile: React Native, Swift, Kotlin. Cloud: AWS, Azure, Docker, Kubernetes. Databases: PostgreSQL, MongoDB, Redis. AI/ML: OpenAI API, LangChain, vector databases.\n",
      "================================================================================\n",
      "\n",
      "❓ Question: How big is the Framna team?\n",
      "\n",
      "💡 Answer: Framna has grown to over 30 employees, including developers, designers, product managers, and data scientists.\n",
      "\n",
      "📚 Q&A pairs used:\n",
      "\n",
      "  1. Q: How many employees does Framna have?\n",
      "A: Framna has grown to over 30 employees including developers, designers, product managers, and data scientists.\n",
      "\n",
      "  2. Q: When was Framna founded?\n",
      "A: Framna was founded in 2019.\n",
      "================================================================================\n",
      "\n",
      "❓ Question: What makes Framna environmentally conscious?\n",
      "\n",
      "💡 Answer: Framna is environmentally conscious because it implements carbon-neutral hosting for all client projects and follows sustainable and ethical business practices. These efforts are part of the company’s commitment to minimizing its environmental impact and supporting sustainability.\n",
      "\n",
      "📚 Q&A pairs used:\n",
      "\n",
      "  1. Q: What makes Framna environmentally conscious?\n",
      "A: Framna implements carbon-neutral hosting for all client projects and follows sustainable and ethical business practices.\n",
      "\n",
      "  2. Q: What are Framna's company values?\n",
      "A: 1. Innovation through technology, 2. Sustainable and ethical business practices, 3. Collaborative teamwork, 4. Continuous learning and development, 5. Customer-centric approach.\n",
      "================================================================================\n",
      "\n",
      "❓ Question: Where can Framna employees work from?\n",
      "\n",
      "💡 Answer: Framna employees can work from anywhere in Europe, as the company has a remote-first work policy that allows its employees this flexibility. Although Framna is based in Stockholm, Sweden, employees are not limited to working from that location.\n",
      "\n",
      "📚 Q&A pairs used:\n",
      "\n",
      "  1. Q: Where is Framna located?\n",
      "A: Framna is based in Stockholm, Sweden with a remote-first work policy allowing employees to work from anywhere in Europe.\n",
      "\n",
      "  2. Q: What is Framna's remote work policy?\n",
      "A: Framna has a remote-first work policy allowing employees to work from anywhere in Europe.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def rag_query(question: str, n_results: int = 2) -> str:\n",
    "    \"\"\"Complete RAG pipeline: retrieve relevant Q&A pairs and generate answer\"\"\"\n",
    "    \n",
    "    # Step 1: Retrieve relevant Q&A pairs\n",
    "    search_results = search_knowledge(question, n_results=n_results)\n",
    "    relevant_qas = search_results['documents'][0]\n",
    "    \n",
    "    # Step 2: Combine Q&A pairs into context\n",
    "    context = \"\\n\\n\".join(relevant_qas)\n",
    "    \n",
    "    # Step 3: Generate answer using context\n",
    "    prompt_template = \"\"\"Based on the following Q&A pairs about Framna, please answer the question. Use the information from the Q&A pairs to provide a comprehensive answer.\n",
    "\n",
    "Relevant Q&A pairs:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    formatted_prompt = prompt_template.format(\n",
    "        context=context,\n",
    "        question=question\n",
    "    )\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4.1\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": formatted_prompt}\n",
    "        ],\n",
    "        max_tokens=300\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content, relevant_qas\n",
    "\n",
    "# Test the RAG system with Q&A database\n",
    "test_questions = [\n",
    "    \"What technology stack does Framna use for mobile development?\",\n",
    "    \"How big is the Framna team?\",\n",
    "    \"What makes Framna environmentally conscious?\",\n",
    "    \"Where can Framna employees work from?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\"\\n❓ Question: {question}\")\n",
    "    answer, qas_used = rag_query(question)\n",
    "    \n",
    "    print(f\"\\n💡 Answer: {answer}\")\n",
    "    \n",
    "    print(f\"\\n📚 Q&A pairs used:\")\n",
    "    for i, qa in enumerate(qas_used):\n",
    "        print(f\"\\n  {i+1}. {qa}\")\n",
    "    \n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Context Injection**: We can provide external knowledge to LLMs through context\n",
    "2. **Scaling Problems**: Large documents/databases are expensive and inefficient to process entirely\n",
    "3. **RAG Solution**: Retrieve only relevant portions using semantic search\n",
    "4. **Q&A Database Pattern**: Perfect example of why RAG is needed - we had 20 Q&A pairs but only needed 2-3 for any given question\n",
    "5. **Embeddings**: Vector representations enable semantic similarity search beyond keyword matching\n",
    "6. **Efficiency**: RAG dramatically reduces token costs while maintaining answer quality\n",
    "\n",
    "## Next Steps\n",
    "- Experiment with different retrieval strategies (top-k, similarity thresholds)\n",
    "- Try different embedding models (text-embedding-3-large vs small)\n",
    "- Add metadata filtering (categories, dates, etc.)\n",
    "- Implement re-ranking of retrieved results\n",
    "- Add evaluation metrics for RAG quality (relevance, completeness)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
