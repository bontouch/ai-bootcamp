{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodality Workshop - Session 1\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "In this workshop, you'll learn how to work with multiple types of content (text, images, audio, video) using OpenAI's API. We'll look at:\n",
    "\n",
    "1. **Text + Image combinations** - How to structure multimodal messages\n",
    "2. **Audio transcription** - Using Whisper for speech-to-text\n",
    "3. **Video processing** - Extracting frames and audio from videos\n",
    "4. **Practical application** - Building a Swish payment parser that processes video content into a Swish payment URL\n",
    "\n",
    "## What is Multimodality?\n",
    "\n",
    "So far, we've worked with text-only AI interactions. **Multimodality** means AI models can understand and work with different types of content simultaneously:\n",
    "\n",
    "- **Text** - Written instructions, descriptions, tabular data, etc.\n",
    "- **Images** - Photos, screenshots, diagrams, charts\n",
    "- **Audio** - Speech, music, sounds (converted to text via transcription)\n",
    "- **Video** - Moving images + audio (processed as frames + transcription)\n",
    "\n",
    "## Beyond this notebook\n",
    "One thing to note is that there are other ways to process multimodal input. In particular video and audio. For instance, OpenAI has a \"Realtime\" API that handles speech directly into GPT-4o. Another example is Google's Gemini Live API that handles text, audi, and video directly and that outputs text and audio.\n",
    "\n",
    "These APIs are still rather expensive and not practical to use in production from an economical perspective. However, we encourage you to explore them. For the right use case, they might very well be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from the uv dependencies you will need ffmpeg installed on your computer. It can be installed easily with:\n",
    "\n",
    "`brew install ffmpeg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from openai import OpenAI\n",
    "from utils import encode_image, extract_audio, sample_frames\n",
    "from contacts import CONTACTS\n",
    "from payment_models import PaymentRequest, ProcessedPayment, evaluate_expression\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text + Image Basics\n",
    "\n",
    "Let's start by understanding how to combine text and images in API requests. There are three ways to provide images to OpenAI models:\n",
    "\n",
    "1. **URL** - Link to an image on the internet\n",
    "2. **Base64** - Encode local images as base64 strings\n",
    "3. **File ID** - Upload images using the Files API\n",
    "\n",
    "### Method 1: Using Image URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Analyze an image from a URL\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"input_text\", \"text\": \"What's in this image? Describe it in detail.\"},\n",
    "                {\n",
    "                    \"type\": \"input_image\",\n",
    "                    \"image_url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Using Base64 Encoded Images\n",
    "\n",
    "For local images, we can encode them as base64 strings. This is useful when you have images stored on your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with base64 encoding (you'll need to add your own image)\n",
    "\n",
    "image_path = \"data/framnacon.png\"\n",
    "base64_image = encode_image(image_path)\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"input_text\", \"text\": \"What's in this image?\"},\n",
    "                {\n",
    "                    \"type\": \"input_image\",\n",
    "                    \"image_url\": f\"data:image/png;base64,{base64_image}\",\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3: Using File IDs\n",
    "\n",
    "You can also upload files using the Files API and reference them by ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file(file_path):\n",
    "    with open(file_path, \"rb\") as file_content:\n",
    "        result = client.files.create(\n",
    "            file=file_content,\n",
    "            purpose=\"vision\",\n",
    "        )\n",
    "        return result.id\n",
    "\n",
    "file_id = create_file(\"data/framnacon.png\")\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"input_text\", \"text\": \"what's in this image?\"},\n",
    "                {\n",
    "                    \"type\": \"input_image\",\n",
    "                    \"file_id\": file_id,\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Images in One Request\n",
    "\n",
    "You can process multiple images in a single request by including them in the content array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Compare two images\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"input_text\", \"text\": \"Compare these two nature scenes. What are the similarities and differences?\"},\n",
    "                {\n",
    "                    \"type\": \"input_image\",\n",
    "                    \"image_url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"input_image\",\n",
    "                    \"image_url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/c/c2/Boardwalk_-_Jeseniky%2C_Czech_Republic_25.jpg/640px-Boardwalk_-_Jeseniky%2C_Czech_Republic_25.jpg\",\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Audio Transcription with Whisper\n",
    "\n",
    "OpenAI's Whisper model can convert speech to text. This is useful for processing audio content or the audio track from videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(audio_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Transcribe audio using OpenAI's Whisper model.\n",
    "    \"\"\"\n",
    "    with open(audio_path, \"rb\") as audio_file:\n",
    "        transcript = client.audio.transcriptions.create(\n",
    "            model=\"whisper-1\", \n",
    "            file=audio_file\n",
    "        )\n",
    "    return transcript.text\n",
    "\n",
    "# Example usage (uncomment when you have an audio file):\n",
    "audio_path = \"cola.mp4\"\n",
    "transcription = transcribe_audio(audio_path)\n",
    "print(f\"Transcription: {transcription}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video Processing\n",
    "To process a video we will:\n",
    "\n",
    "1. **Extract frames** at regular intervals\n",
    "2. **Extract audio** and transcribe it\n",
    "3. **Combine both** in our AI analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import base64\n",
    "\n",
    "def process_video(video_path: str, max_frames: int = 5):\n",
    "    encoded_frames = sample_frames(video_path, max_frames)\n",
    "    audio_path = extract_audio(video_path)\n",
    "    transcription = transcribe_audio(audio_path)\n",
    "    \n",
    "    if os.path.exists(audio_path):\n",
    "        os.remove(audio_path)\n",
    "    \n",
    "    return encoded_frames, transcription\n",
    "\n",
    "video_path = \"data/cola.mp4\"\n",
    "frames, transcript = process_video(video_path)\n",
    "print(f\"Transcript: {transcript}\")\n",
    "\n",
    "def decode_frame(frame_dict):\n",
    "    \"\"\"Decode a frame from the API-style dict with base64-encoded image.\"\"\"\n",
    "    b64_data = frame_dict[\"image_url\"][\"url\"].split(\",\")[1]\n",
    "    img_bytes = base64.b64decode(b64_data)\n",
    "    arr = np.frombuffer(img_bytes, np.uint8)\n",
    "    return cv2.imdecode(arr, cv2.IMREAD_COLOR)\n",
    "\n",
    "decoded_frames = [decode_frame(f) for f in frames]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(decoded_frames), figsize=(15, 3))\n",
    "for i, (ax, img) in enumerate(zip(axes, decoded_frames)):\n",
    "    ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    ax.set_title(f\"Frame {i+1}\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Application - Swish Payment Parser\n",
    "\n",
    "Now let's combine this into a simple application. We'll create a system that can:\n",
    "\n",
    "1. **Process video content** (like someone speaking payment instructions)\n",
    "2. **Extract payment details** (recipient, amount, message)\n",
    "3. **Handle complex amounts** (like \"split the bill of 240 SEK between three people\")\n",
    "4. **Match contacts** from a contact list\n",
    "\n",
    "Note that some of the code is found in the scripts `contacts.py`, `payment_models.py`, and `utils.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_payment_from_video(instructions: str, video_path: str) -> ProcessedPayment:\n",
    "    \"\"\"\n",
    "    Parse payment information from a video by analyzing both visual frames\n",
    "    and audio transcription.\n",
    "    \n",
    "    This demonstrates the full multimodal pipeline:\n",
    "    1. Extract frames from video\n",
    "    2. Extract and transcribe audio\n",
    "    3. Send both to AI for structured analysis\n",
    "    4. Evaluate arithmetic expressions\n",
    "    \"\"\"\n",
    "    encoded_frames, transcript = process_video(video_path, max_frames=5)\n",
    "    \n",
    "    content = [\n",
    "        {\"type\": \"text\", \"text\": \"# Key video frames:\"},\n",
    "        encoded_frames[0],\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"text\": f\"\"\"\n",
    "            # User Contacts\n",
    "            {CONTACTS}\n",
    "\n",
    "            # Video Transcript\n",
    "            {transcript}\n",
    "\n",
    "            # User Instructions\n",
    "            {instructions}\n",
    "            \"\"\",\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    response = client.chat.completions.parse(\n",
    "        model=\"gpt-4.1\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are an AI assistant that analyzes video content to extract \"\n",
    "                    \"structured information for digital payments (Swish). \"\n",
    "                    \"You will be provided with key frames from the video, a transcript \"\n",
    "                    \"of the audio, user instructions, and contact list. \"\n",
    "                    \"Represent amounts as arithmetic expressions using +, -, *, /.\"\n",
    "                ),\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": content},\n",
    "        ],\n",
    "        response_format=PaymentRequest,\n",
    "    )\n",
    "\n",
    "    payment_request: PaymentRequest = response.choices[0].message.parsed\n",
    "    print(f\"Extracted payment request: {payment_request}\")\n",
    "    \n",
    "    # Evaluate the arithmetic expression\n",
    "    evaluated_amount = evaluate_expression(payment_request.expression)\n",
    "    print(f\"Evaluated amount: {evaluated_amount}\")\n",
    "    \n",
    "    return ProcessedPayment(\n",
    "        phone_number=payment_request.phone_number,\n",
    "        amount=evaluated_amount,\n",
    "        message=payment_request.message,\n",
    "    )\n",
    "\n",
    "video_path = \"data/cola.mp4\"\n",
    "result = parse_payment_from_video(\n",
    "    \"Process this payment request\",\n",
    "    video_path\n",
    ")\n",
    "print(\"\\n=== RESULTS ===\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct URL\n",
    "With the data transformed into our data model we can easily turn it into an URL that can be used to deeplink us into the Swish app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import quote_plus\n",
    "\n",
    "def build_swish_url(payment: ProcessedPayment) -> str:\n",
    "    base_url = \"https://app.swish.nu/1/p/sw/?\"\n",
    "\n",
    "    encoded_message = quote_plus(payment.message)\n",
    "\n",
    "    params = (\n",
    "        f\"sw={payment.phone_number}&\"\n",
    "        f\"amt={round(payment.amount, 1)}&\"\n",
    "        f\"cur=SEK&\"\n",
    "        f\"msg={encoded_message}\"\n",
    "    )\n",
    "\n",
    "    return base_url + params\n",
    "\n",
    "url = build_swish_url(result)\n",
    "print(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Including arithmetics\n",
    "The structured output models also contains a setup for handling arithmetic operations if requested by the LLM. Below is an example only with text.\n",
    "\n",
    "Try recording your own video where you prompt the model to, for instance, sum/subtract something together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_payment_from_text(text: str) -> ProcessedPayment:\n",
    "    \"\"\"\n",
    "    Parse payment information from a text prompt only.\n",
    "    \n",
    "    This version removes all video/audio processing and instead relies\n",
    "    purely on the provided text + instructions.\n",
    "    \"\"\"\n",
    "    content = f\"\"\"\n",
    "    # User Contacts\n",
    "    {CONTACTS}\n",
    "\n",
    "    # Payment Text\n",
    "    {text}\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.parse(\n",
    "        model=\"gpt-4.1\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are an AI assistant that extracts structured payment \"\n",
    "                    \"information (Swish). You will be provided with a contact list, \"\n",
    "                    \"a text description of the payment, and user instructions. \"\n",
    "                    \"Represent amounts as arithmetic expressions using +, -, *, /.\"\n",
    "                ),\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": content},\n",
    "        ],\n",
    "        response_format=PaymentRequest,\n",
    "    )\n",
    "\n",
    "    payment_request: PaymentRequest = response.choices[0].message.parsed\n",
    "    print(f\"Extracted payment request: {payment_request}\")\n",
    "\n",
    "    evaluated_amount = evaluate_expression(payment_request.expression)\n",
    "    print(f\"Evaluated amount: {evaluated_amount}\")\n",
    "\n",
    "    return ProcessedPayment(\n",
    "        phone_number=payment_request.phone_number,\n",
    "        amount=evaluated_amount,\n",
    "        message=payment_request.message,\n",
    "    )\n",
    "\n",
    "\n",
    "# Example usage\n",
    "text_input = \"Pay Anna 2 bottles of cola for 15 SEK\"\n",
    "result = parse_payment_from_text(text_input)\n",
    "\n",
    "print(\"\\n=== RESULTS ===\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with Example Data\n",
    "\n",
    "For debugging purposes, we can test out the operations. Here below we test two sets of operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the expression evaluation system\n",
    "from payment_models import Number, BinaryOperation\n",
    "\n",
    "# Example: 240 / 3 (splitting a bill)\n",
    "test_expression = BinaryOperation(\n",
    "    type=\"binary_op\",\n",
    "    op=\"/\",\n",
    "    left=Number(type=\"number\", value=240),\n",
    "    right=Number(type=\"number\", value=3)\n",
    ")\n",
    "\n",
    "result = evaluate_expression(test_expression)\n",
    "print(f\"240 / 3 = {result}\")\n",
    "\n",
    "# Example: (100 + 50) * 0.5 (adding items then taking half)\n",
    "complex_expression = BinaryOperation(\n",
    "    type=\"binary_op\",\n",
    "    op=\"*\",\n",
    "    left=BinaryOperation(\n",
    "        type=\"binary_op\",\n",
    "        op=\"+\",\n",
    "        left=Number(type=\"number\", value=100),\n",
    "        right=Number(type=\"number\", value=50)\n",
    "    ),\n",
    "    right=Number(type=\"number\", value=0.5)\n",
    ")\n",
    "\n",
    "result = evaluate_expression(complex_expression)\n",
    "print(f\"(100 + 50) * 0.5 = {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Takeaways\n",
    "\n",
    "**And that's it!** You've learned the basics of multimodal AI:\n",
    "\n",
    "### What We Covered:\n",
    "\n",
    "1. **Multimodal Message Structure** - How to combine text, images, and transcribed audio\n",
    "1. **Image Input Methods** - URL, base64, and file upload approaches\n",
    "1. **Audio Processing** - Using Whisper for speech-to-text transcription"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
