{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product Image Similarity Search with CLIP\n",
    "\n",
    "Pure visual similarity matching using CLIP embeddings:\n",
    "1. Generate CLIP embeddings for all product images\n",
    "2. Find visually similar products using cosine similarity\n",
    "3. Optional: Use GPT-4.1 for final verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dependencies\n",
    "\n",
    "Install required packages for CLIP model and image processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to install the necessary packages for using the CLIP model\n",
    "!pip install transformers torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Models and Libraries\n",
    "### CLIP Model\n",
    "Embeddings for images are offered in the API by some providers (Google, for instance) but unfortunately not OpenAI. To avoid having to deal with Google service accounts and access we'll use the local CLIP model.\n",
    "\n",
    "CLIP is a model from OpenAI that links images and text in a shared representation space. It turns images into embeddings—numerical vectors that capture semantic meaning—so we can measure how similar two images are.\n",
    "\n",
    "We use CLIP because it allows us to perform similarity search on images: given a query image, we can find other images that look alike or share visual characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Initialize\n",
    "client = OpenAI()\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model.to(device)\n",
    "\n",
    "print(f\"✅ CLIP model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimilarityMatch(BaseModel):\n",
    "    image_id: str\n",
    "    is_same_product: bool\n",
    "    confidence_score: float = Field(ge=0.0, le=1.0)\n",
    "    reasoning: str\n",
    "\n",
    "class ProductMatchResult(BaseModel):\n",
    "    query_image_path: str\n",
    "    matches: List[SimilarityMatch]\n",
    "    best_match_id: Optional[str]\n",
    "    summary: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clip_embedding(image_path: str) -> np.ndarray:\n",
    "    \"\"\"Generate CLIP visual embedding for an image\"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.get_image_features(**inputs)\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    return image_features.cpu().numpy().flatten()\n",
    "\n",
    "def encode_image(image_path: str) -> str:\n",
    "    \"\"\"Encode image as base64 for OpenAI API\"\"\"\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        return base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "\n",
    "print(\"✅ Helper functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product Database\n",
    "We initialize a product database for storing embeddings and doing similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductDatabase:\n",
    "    def __init__(self):\n",
    "        self.products = {}\n",
    "        self.embeddings = {}\n",
    "    \n",
    "    def add_product(self, product_id: str, image_path: str):\n",
    "        self.products[product_id] = {\"id\": product_id, \"image_path\": image_path}\n",
    "        self.embeddings[product_id] = get_clip_embedding(image_path)\n",
    "        print(f\"Added {product_id}\")\n",
    "    \n",
    "    def find_similar(self, query_embedding: np.ndarray, top_k: int = 5) -> List[str]:\n",
    "        if not self.embeddings:\n",
    "            return []\n",
    "        \n",
    "        ids = list(self.embeddings.keys())\n",
    "        vectors = np.stack([self.embeddings[pid] for pid in ids])\n",
    "        similarities = cosine_similarity([query_embedding], vectors)[0]\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        \n",
    "        return [ids[i] for i in top_indices]\n",
    "    \n",
    "    def get_similarities(self, query_embedding: np.ndarray, product_ids: List[str]) -> List[float]:\n",
    "        vectors = np.stack([self.embeddings[pid] for pid in product_ids if pid in self.embeddings])\n",
    "        if len(vectors) == 0:\n",
    "            return []\n",
    "        return cosine_similarity([query_embedding], vectors)[0].tolist()\n",
    "    \n",
    "    def size(self) -> int:\n",
    "        return len(self.products)\n",
    "\n",
    "db = ProductDatabase()\n",
    "print(\"✅ Database ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir = Path(\"sample_product_images\")\n",
    "image_files = list(images_dir.glob(\"*.jpg\")) + list(images_dir.glob(\"*.png\"))\n",
    "\n",
    "print(f\"Found {len(image_files)} images\")\n",
    "print(\"Building database...\")\n",
    "\n",
    "for img_path in image_files:\n",
    "    db.add_product(img_path.stem, str(img_path))\n",
    "\n",
    "print(f\"✅ Database built with {db.size()} products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_products(query_image_path: str, top_k: int = 5) -> ProductMatchResult:\n",
    "    \"\"\"Find similar products using CLIP embeddings\"\"\"\n",
    "    \n",
    "    # Get CLIP embedding for query\n",
    "    query_embedding = get_clip_embedding(query_image_path)\n",
    "    \n",
    "    # Find similar products\n",
    "    similar_ids = db.find_similar(query_embedding, top_k)\n",
    "    similarities = db.get_similarities(query_embedding, similar_ids)\n",
    "    \n",
    "    # Create matches\n",
    "    matches = []\n",
    "    for i, product_id in enumerate(similar_ids):\n",
    "        score = similarities[i] if i < len(similarities) else 0.0\n",
    "        is_match = score > 0.85\n",
    "        \n",
    "        matches.append(SimilarityMatch(\n",
    "            image_id=product_id,\n",
    "            is_same_product=is_match,\n",
    "            confidence_score=min(score, 0.95),\n",
    "            reasoning=f\"CLIP similarity: {score:.3f}\"\n",
    "        ))\n",
    "    \n",
    "    best_match = next((m.image_id for m in matches if m.is_same_product), None)\n",
    "    \n",
    "    return ProductMatchResult(\n",
    "        query_image_path=query_image_path,\n",
    "        matches=matches,\n",
    "        best_match_id=best_match,\n",
    "        summary=f\"Found {len([m for m in matches if m.is_same_product])} potential matches\"\n",
    "    )\n",
    "\n",
    "print(\"✅ Search function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "## Simple similarity test\n",
    "First we do a simple test to see if the similarity search works. We test with an image we know exists in the database already. This should lead to a very high matching score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_image = image_files[5]\n",
    "\n",
    "print(f\"🔍 Testing with: {query_image.name}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "result = find_similar_products(str(query_image))\n",
    "\n",
    "print(f\"Summary: {result.summary}\")\n",
    "print(\"\\nTop matches:\")\n",
    "\n",
    "for i, match in enumerate(result.matches, 1):\n",
    "    status = \"✅ MATCH\" if match.is_same_product else \"❌ NO MATCH\"\n",
    "    print(f\"{i}. {match.image_id}: {status} ({match.confidence_score:.3f})\")\n",
    "\n",
    "if result.best_match_id:\n",
    "    print(f\"\\n🏆 Best match: {result.best_match_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity search together with GPT-4.1\n",
    "Now we will use GPT-4.1 to analyze if the most similar candidates match the query. So we do a rough similarity search first, and then we let GPT-4.1 reason if any are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_with_gpt(query_image_path: str, candidate_paths: List[str], candidate_ids: List[str]) -> ProductMatchResult:    \n",
    "    content = [{\n",
    "        \"type\": \"input_text\",\n",
    "        \"text\": f\"Compare the first image to these candidates: {candidate_ids}. Which show the same exact product?\"\n",
    "    }]\n",
    "    \n",
    "    # Add query image\n",
    "    content.append({\n",
    "        \"type\": \"input_image\",\n",
    "        \"image_url\": f\"data:image/jpeg;base64,{encode_image(query_image_path)}\"\n",
    "    })\n",
    "    \n",
    "    # Add candidate images\n",
    "    for i, img_path in enumerate(candidate_paths):\n",
    "        content.extend([\n",
    "            {\"type\": \"input_text\", \"text\": f\"Candidate {candidate_ids[i]}:\"},\n",
    "            {\"type\": \"input_image\", \"image_url\": f\"data:image/jpeg;base64,{encode_image(img_path)}\"}\n",
    "        ])\n",
    "    \n",
    "    response = client.responses.parse(\n",
    "        model=\"gpt-4.1\",\n",
    "        input=[{\"role\": \"user\", \"content\": content}],\n",
    "        text_format=ProductMatchResult\n",
    "    )\n",
    "    \n",
    "    return response.output_parsed\n",
    "\n",
    "print(\"✅ GPT-4.1 reasoning function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test with an example image of a bottle of Pernod that we know exist in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image as NotebookImage\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#query_image = image_files[3]\n",
    "query_image = Path(\"pernod.png\")\n",
    "\n",
    "print(\"🤖 TESTING GPT-4.1 MULTI-MODAL REASONING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "clip_result = find_similar_products(str(query_image), top_k=5)\n",
    "\n",
    "print(f\"📸 Query: {query_image.name}\")\n",
    "display(NotebookImage(filename=str(query_image)))\n",
    "\n",
    "print(f\"🔍 CLIP found {len(clip_result.matches)} similar images\")\n",
    "\n",
    "candidate_paths = []\n",
    "candidate_ids = []\n",
    "\n",
    "for match in clip_result.matches:\n",
    "    img_path = db.products[match.image_id][\"image_path\"]\n",
    "    candidate_paths.append(img_path)\n",
    "    candidate_ids.append(match.image_id)\n",
    "\n",
    "# Display candidates side by side\n",
    "fig, axes = plt.subplots(1, len(candidate_paths), figsize=(5 * len(candidate_paths), 5))\n",
    "if len(candidate_paths) == 1:\n",
    "    axes = [axes]\n",
    "for ax, path, cid in zip(axes, candidate_paths, candidate_ids):\n",
    "    ax.imshow(Image.open(path))\n",
    "    ax.set_title(cid)\n",
    "    ax.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n🧠 Sending to GPT-4.1 for reasoning...\")\n",
    "\n",
    "try:\n",
    "    gpt_result = analyze_with_gpt(str(query_image), candidate_paths, candidate_ids)\n",
    "    \n",
    "    print(\"\\n📊 GPT-4.1 RESULTS:\")\n",
    "    print(f\"Summary: {gpt_result.summary}\")\n",
    "    \n",
    "    print(\"\\nDetailed analysis:\")\n",
    "    for match in gpt_result.matches:\n",
    "        status = \"✅ MATCH\" if match.is_same_product else \"❌ NO MATCH\"\n",
    "        print(f\"  {match.image_id}: {status} (confidence: {match.confidence_score:.3f})\")\n",
    "        print(f\"    Reasoning: {match.reasoning}\")\n",
    "    \n",
    "    if gpt_result.best_match_id:\n",
    "        print(f\"\\n🏆 GPT-4.1 selected: {gpt_result.best_match_id}\")\n",
    "    else:\n",
    "        print(\"\\n❌ GPT-4.1 found no exact matches\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error with GPT-4.1 analysis: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
