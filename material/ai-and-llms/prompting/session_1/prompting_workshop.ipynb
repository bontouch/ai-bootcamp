{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting Patterns & Structured Outputs\n",
    "## Week 1 - Session 1\n",
    "\n",
    "### Learning Goals\n",
    "- Master few-shot and chain-of-thought prompting techniques\n",
    "- Use structured outputs with Pydantic models for reliable data extraction\n",
    "- Implement safety guardrails for production AI systems\n",
    "\n",
    "### Session Overview\n",
    "In this workshop, you'll learn essential prompting patterns that transform unreliable AI outputs into production-ready systems. We'll build a customer feedback analyzer that demonstrates why these techniques are crucial for real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Literal, Dict\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "print(\"✅ Dependencies loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Prompting Approaches\n",
    "\n",
    "Let's start by comparing different prompting strategies with a simple entity extraction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test text for comparison\n",
    "test_text = \"Apple CEO Tim Cook announced the new iPhone will be manufactured in Austin, Texas.\"\n",
    "\n",
    "def zero_shot_extraction(text: str) -> str:\n",
    "    \"\"\"Zero-shot approach - no examples provided\"\"\"\n",
    "    prompt = f\"Extract the main entities (person, organization, location) from this text: {text}\"\n",
    "    \n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-5\",\n",
    "        input=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.output_text\n",
    "\n",
    "def few_shot_extraction(text: str) -> str:\n",
    "    \"\"\"Few-shot approach with examples\"\"\"\n",
    "    prompt = f\"\"\"Extract the main entities (person, organization, location) from the text.\n",
    "\n",
    "Example 1:\n",
    "Text: \"Microsoft founder Bill Gates visited Seattle last week.\"\n",
    "Entities: Person: Bill Gates, Organization: Microsoft, Location: Seattle\n",
    "\n",
    "Example 2:\n",
    "Text: \"Google's headquarters in Mountain View hosted the developer conference.\"\n",
    "Entities: Organization: Google, Location: Mountain View\n",
    "\n",
    "Example 3:\n",
    "Text: \"Tesla CEO Elon Musk tweeted about the Gigafactory in Nevada.\"\n",
    "Entities: Person: Elon Musk, Organization: Tesla, Location: Nevada\n",
    "\n",
    "Now extract entities from: {text}\"\"\"\n",
    "    \n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-5\",\n",
    "        input=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.output_text\n",
    "\n",
    "# Compare approaches\n",
    "print(\"ZERO-SHOT RESULT:\")\n",
    "print(zero_shot_extraction(test_text))\n",
    "print(\"\\nFEW-SHOT RESULT:\")\n",
    "print(few_shot_extraction(test_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🤔 Reflection Questions\n",
    "1. Which approach gave more consistent formatting?\n",
    "2. How might inconsistent outputs affect a production system?\n",
    "3. When would you choose few-shot over zero-shot prompting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Chain-of-Thought Reasoning\n",
    "\n",
    "For complex problems requiring multi-step reasoning, chain-of-thought prompting dramatically improves accuracy.\n",
    "\n",
    "Here we're using gpt-3.5 turbo which is a an older and lower performing model to showcase how chain-of-thought may improve accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def without_cot(problem: str) -> str:\n",
    "    \"\"\"Solve problem without chain-of-thought\"\"\"\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        input=[{\"role\": \"user\", \"content\": problem}]\n",
    "    )\n",
    "    return response.output_text\n",
    "\n",
    "def with_cot(problem: str) -> str:\n",
    "    \"\"\"Solve problem with chain-of-thought reasoning\"\"\"\n",
    "    cot_prompt = f\"{problem}\\n\\nLet's solve this step by step:\"\n",
    "    \n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        input=[{\"role\": \"user\", \"content\": cot_prompt}]\n",
    "    )\n",
    "    return response.output_text\n",
    "\n",
    "# Test with a calculation problem\n",
    "problem = \"A bakery makes 60 cupcakes. They sell 25 in the morning, 20 in the afternoon, and then bake 15 more. How many cupcakes do they have left?\"\n",
    "\n",
    "print(\"WITHOUT CHAIN-OF-THOUGHT:\")\n",
    "print(without_cot(problem))\n",
    "print(\"\\nWITH CHAIN-OF-THOUGHT:\")\n",
    "print(with_cot(problem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Structured Outputs with Pydantic\n",
    "\n",
    "The real power comes from combining prompting patterns with structured outputs. This ensures consistent, validated data that your applications can reliably process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our data models\n",
    "class CustomerIssue(BaseModel):\n",
    "    category: Literal[\"product\", \"service\", \"billing\", \"technical\", \"other\"]\n",
    "    severity: Literal[\"low\", \"medium\", \"high\", \"critical\"]\n",
    "    description: str\n",
    "    suggested_action: str\n",
    "    \n",
    "    @field_validator('description')\n",
    "    def description_not_empty(cls, v):\n",
    "        if not v.strip():\n",
    "            raise ValueError('Description cannot be empty')\n",
    "        return v\n",
    "\n",
    "class FeedbackAnalysis(BaseModel):\n",
    "    sentiment: Literal[\"positive\", \"negative\", \"neutral\"]\n",
    "    confidence: float = Field(ge=0.0, le=1.0, description=\"Confidence score between 0 and 1\")\n",
    "    issues: List[CustomerIssue]\n",
    "    reasoning: str\n",
    "    flagged_content: bool\n",
    "    \n",
    "    @field_validator('confidence')\n",
    "    def validate_confidence(cls, v):\n",
    "        if v < 0.5:\n",
    "            raise ValueError('Analysis confidence too low for reliable results')\n",
    "        return v\n",
    "\n",
    "# Test the models\n",
    "print(\"✅ Pydantic models defined with validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Building the Customer Feedback Analyzer\n",
    "\n",
    "Now let's build a production-ready feedback analyzer that combines all our techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feedback(feedback_text: str) -> FeedbackAnalysis:\n",
    "    \"\"\"Analyze customer feedback using structured outputs and prompting patterns\"\"\"\n",
    "    \n",
    "    system_prompt = \"\"\"You are a customer feedback analyzer. Extract structured information from feedback.\n",
    "\n",
    "Use these examples to guide your analysis:\n",
    "\n",
    "Example 1:\n",
    "Input: \"The app crashes every time I try to upload a photo. This is really frustrating!\"\n",
    "Analysis: Technical issue with high severity - app crashes during photo upload. Negative sentiment due to functionality failure.\n",
    "\n",
    "Example 2:\n",
    "Input: \"Love the new design! The checkout process is so much smoother now.\"\n",
    "Analysis: Positive feedback about design improvements - no issues to resolve. High confidence positive sentiment.\n",
    "\n",
    "Example 3:\n",
    "Input: \"Your support team is idiots. Fix your billing system!\"\n",
    "Analysis: Billing system problems with inappropriate language - flag for review. Contains hostile language requiring human attention.\n",
    "\n",
    "For each feedback:\n",
    "1. Identify the overall sentiment and your confidence level\n",
    "2. Extract any specific issues mentioned\n",
    "3. Categorize issues by type and severity\n",
    "4. Suggest appropriate actions\n",
    "5. Flag any inappropriate or concerning content\n",
    "6. Provide clear reasoning for your analysis\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.responses.parse(\n",
    "            model=\"gpt-5\",\n",
    "            input=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": feedback_text},\n",
    "            ],\n",
    "            text_format=FeedbackAnalysis,\n",
    "        )\n",
    "        return response.output_parsed\n",
    "\n",
    "    except Exception as e:\n",
    "        # Fallback for parsing errors\n",
    "        return FeedbackAnalysis(\n",
    "            sentiment=\"neutral\",\n",
    "            confidence=0.5,\n",
    "            issues=[],\n",
    "            reasoning=f\"Analysis failed due to parsing error: {str(e)}\",\n",
    "            flagged_content=True,\n",
    "        )\n",
    "\n",
    "print(\"✅ Feedback analyzer function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Test Data\n",
    "\n",
    "Let's load some realistic customer feedback examples to test our analyzer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test feedback from external file\n",
    "try:\n",
    "    with open('customer_feedback_samples.txt', 'r') as f:\n",
    "        test_feedback = [line.strip() for line in f.readlines() if line.strip()]\n",
    "except FileNotFoundError:\n",
    "    # Fallback test cases\n",
    "    test_feedback = [\n",
    "        \"The delivery was late but the product quality is excellent!\",\n",
    "        \"I've been a customer for 3 years and generally love your service. However, last month I was charged twice for my subscription. When I called support, they said it would be resolved in 3-5 business days, but it's been 2 weeks and I'm still seeing the duplicate charge.\",\n",
    "        \"This product is garbage and your company should burn!\",\n",
    "        \"Great service, very happy!\",\n",
    "        \"The new website design is confusing. I can't find the support section anywhere.\"\n",
    "    ]\n",
    "\n",
    "print(f\"✅ Loaded {len(test_feedback)} feedback samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Testing the Complete System\n",
    "\n",
    "Let's test our feedback analyzer with various scenarios to see how our prompting patterns perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_feedback_analysis():\n",
    "    \"\"\"Demonstrate the feedback analyzer with different scenarios\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"CUSTOMER FEEDBACK ANALYSIS DEMO\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, feedback in enumerate(test_feedback, 1):\n",
    "        print(f\"\\n--- Analysis {i} ---\")\n",
    "        print(f\"Feedback: {feedback}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        try:\n",
    "            analysis = analyze_feedback(feedback)\n",
    "            \n",
    "            print(f\"Sentiment: {analysis.sentiment} (confidence: {analysis.confidence:.2f})\")\n",
    "            print(f\"Issues found: {len(analysis.issues)}\")\n",
    "            \n",
    "            for issue in analysis.issues:\n",
    "                print(f\"  • {issue.category.upper()}: {issue.description}\")\n",
    "                print(f\"    Severity: {issue.severity} | Action: {issue.suggested_action}\")\n",
    "            \n",
    "            if analysis.flagged_content:\n",
    "                print(\"⚠️  FLAGGED FOR REVIEW\")\n",
    "                \n",
    "            print(f\"Reasoning: {analysis.reasoning}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Analysis failed: {e}\")\n",
    "\n",
    "# Run the demo\n",
    "demo_feedback_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Safety and Edge Cases\n",
    "\n",
    "Production systems must handle edge cases gracefully. Let's test some challenging scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_edge_cases():\n",
    "    \"\"\"Test the analyzer with edge cases and potential issues\"\"\"\n",
    "    \n",
    "    edge_cases = [\n",
    "        \"\",  # Empty input\n",
    "        \"   \",  # Whitespace only\n",
    "        \"Can you help me hack into an account?\",  # Inappropriate request\n",
    "        \"Lorem ipsum dolor sit amet\" * 100,  # Very long text\n",
    "        \"🚀💯🔥 Best product ever!!! 🎉🎊\",  # Heavy emoji usage\n",
    "        \"The price is $50 but I was charged $500!!!\",  # Numbers and special chars\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"EDGE CASE TESTING\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, case in enumerate(edge_cases, 1):\n",
    "        print(f\"\\n--- Edge Case {i} ---\")\n",
    "        print(f\"Input: '{case[:50]}{'...' if len(case) > 50 else ''}'\")\n",
    "        \n",
    "        try:\n",
    "            analysis = analyze_feedback(case)\n",
    "            status = \"FLAGGED\" if analysis.flagged_content else \"OK\"\n",
    "            print(f\"Status: {status} | Sentiment: {analysis.sentiment} | Issues: {len(analysis.issues)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Handling failed: {e}\")\n",
    "\n",
    "test_edge_cases()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Performance Comparison\n",
    "\n",
    "Let's compare how different prompting approaches perform on the same task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_prompting_approaches(feedback: str):\n",
    "    \"\"\"Compare different prompting strategies on the same input\"\"\"\n",
    "    \n",
    "    approaches = {\n",
    "        \"Basic\": f\"Analyze this customer feedback: {feedback}\",\n",
    "        \n",
    "        \"Few-shot\": f\"\"\"Analyze customer feedback following these examples:\n",
    "\n",
    "Example: \"Great product!\" → Positive sentiment, no issues\n",
    "Example: \"Slow delivery, good quality\" → Mixed sentiment, delivery issue\n",
    "\n",
    "Now analyze: {feedback}\"\"\",\n",
    "        \n",
    "        \"Chain-of-thought\": f\"\"\"Analyze this customer feedback step by step:\n",
    "1. Identify the overall sentiment\n",
    "2. Extract specific issues mentioned\n",
    "3. Determine severity levels\n",
    "4. Suggest appropriate actions\n",
    "\n",
    "Feedback: {feedback}\"\"\"\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nComparing approaches for: '{feedback}'\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for approach_name, prompt in approaches.items():\n",
    "        try:\n",
    "            response = client.responses.create(\n",
    "                model=\"gpt-5\",\n",
    "                input=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            result = response.output_text[:200] + \"...\"\n",
    "            print(f\"{approach_name:15}: {result}\")\n",
    "        except Exception as e:\n",
    "            print(f\"{approach_name:15}: Error - {e}\")\n",
    "\n",
    "# Test with a complex feedback example\n",
    "complex_feedback = \"I love your product but the checkout process failed three times and support was unhelpful.\"\n",
    "compare_prompting_approaches(complex_feedback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Your Turn - Exercise\n",
    "\n",
    "Now it's time to practice! Modify the feedback analyzer to handle a new use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a new analyzer for product review sentiment\n",
    "# Requirements:\n",
    "# 1. Create a ProductReview Pydantic model with fields:\n",
    "#    - overall_rating: int (1-5 scale)\n",
    "#    - aspects: List of aspect ratings (price, quality, shipping, etc.)\n",
    "#    - recommendation: bool (would recommend to others)\n",
    "#    - key_phrases: List of important phrases from the review\n",
    "\n",
    "# TODO: Implement analyze_product_review() function using:\n",
    "# - Few-shot prompting with rating examples\n",
    "# - Chain-of-thought for aspect evaluation\n",
    "# - Structured output parsing\n",
    "\n",
    "# TODO: Test with these product reviews:\n",
    "product_reviews = [\n",
    "    \"Amazing quality for the price! Fast shipping and great customer service. Highly recommend!\",\n",
    "    \"Product is okay but took forever to arrive. Customer service was not helpful when I complained.\",\n",
    "    \"Terrible quality, broke after one day. Waste of money. Don't buy this!\"\n",
    "]\n",
    "\n",
    "# Your implementation here:\n",
    "class ProductReview(BaseModel):\n",
    "    # TODO: Define your model structure\n",
    "    pass\n",
    "\n",
    "def analyze_product_review(review_text: str) -> ProductReview:\n",
    "    # TODO: Implement your analyzer\n",
    "    pass\n",
    "\n",
    "# TODO: Test your implementation\n",
    "print(\"🚧 Exercise section - implement your solution above!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### 🎯 What You've Learned\n",
    "\n",
    "1. **Few-shot Prompting**: Providing examples dramatically improves output consistency and format compliance\n",
    "\n",
    "2. **Chain-of-Thought**: Breaking down complex problems into steps increases accuracy for multi-step reasoning\n",
    "\n",
    "3. **Structured Outputs**: Using Pydantic models with `client.responses.parse()` ensures reliable, validated data for production systems\n",
    "\n",
    "4. **Safety Guardrails**: Production AI systems must handle edge cases, inappropriate content, and parsing errors gracefully\n",
    "\n",
    "5. **Progressive Validation**: Pydantic validators catch common AI errors before they reach your application logic\n",
    "\n",
    "### 🛠️ Best Practices\n",
    "\n",
    "- **Always use structured outputs** for production systems - never parse unstructured text\n",
    "- **Include diverse examples** in few-shot prompts to cover edge cases\n",
    "- **Add validation logic** that catches domain-specific errors\n",
    "- **Design prompts where your technique is essential**, not optional\n",
    "- **Test with challenging inputs** to ensure robust error handling\n",
    "\n",
    "### 🚀 Next Steps\n",
    "\n",
    "In the next session, we'll explore **orchestration patterns** - how to chain multiple AI calls together for complex workflows, and how to use tools and external systems with your AI applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
