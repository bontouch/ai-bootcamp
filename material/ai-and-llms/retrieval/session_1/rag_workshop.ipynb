{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval & Context Systems\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the limitations of LLMs without external context\n",
    "- Learn how to inject domain-specific knowledge using context\n",
    "- Explore basic RAG patterns with embeddings and vector search\n",
    "- Practice chunking strategies and retrieval techniques\n",
    "\n",
    "## Setup\n",
    "Make sure you have your OpenAI API key set as an environment variable or in a `.env` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's up with Retrieval and \"RAG\"?\n",
    "\n",
    "**Retrieval** is the fundamental concept of finding and fetching relevant information from external knowledge sources. It's the \"R\" in **RAG (Retrieval-Augmented Generation)** - a pattern that combines:\n",
    "\n",
    "- **Retrieval**: Finding relevant context from external sources (documents, databases, APIs)\n",
    "- **Augmented**: Enhancing the LLM's capabilities with this external knowledge\n",
    "- **Generation**: Using the LLM to generate responses informed by the retrieved context\n",
    "\n",
    "RAG solves a critical limitation: LLMs are trained on static datasets and can't access real-time information or private company data. By retrieving relevant context and injecting it into prompts, we can make LLMs knowledgeable about domains they were never trained on.\n",
    "\n",
    "Today we'll explore various retrieval techniques, from simple context injection to vector-based search systems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from typing import List, Dict\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 1: Testing LLM Knowledge Limits\n",
    "\n",
    "Let's start by asking the model something it won't know - questions about Framna, our company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question: str) -> str:\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-4.1\",\n",
    "        input=[\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ],\n",
    "    )\n",
    "    return response.output[0].content[0].text\n",
    "\n",
    "# Try asking about Framna\n",
    "question = \"What services does Framna offer and what technology stack do they use?\"\n",
    "answer = ask_question(question)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Result:** The model won't know about Framna since it's not in its training data.\n",
    "\n",
    "## Challenge 2: Adding Context - The Naive Approach\n",
    "\n",
    "Now let's load our company information and inject it as context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load company information\n",
    "with open('framna_company_info.txt', 'r', encoding='utf-8') as f:\n",
    "    company_info = f.read()\n",
    "\n",
    "print(f\"Loaded {len(company_info)} characters of company information\")\n",
    "print(f\"First 200 characters: {company_info[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_with_context(question: str, context: str) -> str:\n",
    "    \"\"\"Ask a question with provided context using string formatting\"\"\"\n",
    "    \n",
    "    # Define our prompt template with placeholders\n",
    "    prompt_template = \"\"\"Based on the following company information, please answer the question.\n",
    "\n",
    "Company Information:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer based only on the provided information:\"\"\"\n",
    "    \n",
    "    formatted_prompt = prompt_template.format(\n",
    "        context=context,\n",
    "        question=question\n",
    "    )\n",
    "    \n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-4.1\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": formatted_prompt}\n",
    "        ],\n",
    "        max_tokens=300\n",
    "    )\n",
    "    return response.output[0].content[0].text\n",
    "\n",
    "# Now ask the same question with context\n",
    "answer_with_context = ask_with_context(question, company_info)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer with context: {answer_with_context}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Result:** Now the model can answer accurately about Framna!\n",
    "\n",
    "Let's try a few more questions to see how well this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test more questions\n",
    "questions = [\n",
    "    \"Where is Framna located and what is their work policy?\",\n",
    "    \"What are Framna's company values?\",\n",
    "    \"What notable projects has Framna worked on?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    answer = ask_with_context(q, company_info)\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    print(f\"A: {answer}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 3: The Problem with Large Documents\n",
    "\n",
    "What happens when we have too much information? Let's simulate a larger document and see the limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a much larger document by repeating our content\n",
    "large_document = company_info * 10  # 10x the original size\n",
    "\n",
    "print(f\"Original document: {len(company_info)} characters\")\n",
    "print(f\"Large document: {len(large_document)} characters\")\n",
    "\n",
    "# Calculate approximate token count (rough estimate: 1 token ≈ 4 characters)\n",
    "estimated_tokens = len(large_document) // 0.7\n",
    "print(f\"Estimated tokens: {estimated_tokens}\")\n",
    "\n",
    "# This would be expensive and inefficient!\n",
    "print(\"\\n⚠️ Problems with large documents:\")\n",
    "print(\"- Higher API costs (more tokens)\")\n",
    "print(\"- Slower response times\")\n",
    "print(\"- Context window limits (models have max token limits)\")\n",
    "print(\"- Difficulty finding relevant information (needle in haystack)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 4: Introduction to RAG - Q&A Database Approach\n",
    "\n",
    "Now let's see a more practical example. We have a large Q&A database about Framna. It would be inefficient to send all Q&A pairs for every question. Instead, we'll use RAG to find only the relevant Q&A pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ChromaDB if not already installed\n",
    "# !pip install chromadb\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "chroma_client = chromadb.Client(Settings(anonymized_telemetry=False))\n",
    "\n",
    "# Create a collection\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"framna_knowledge\",\n",
    "    metadata={\"description\": \"Framna company information\"}\n",
    ")\n",
    "\n",
    "print(\"ChromaDB collection created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our Q&A database instead\n",
    "with open('framna_qa_database.txt', 'r', encoding='utf-8') as f:\n",
    "    qa_database = f.read()\n",
    "\n",
    "print(f\"Loaded Q&A database with {len(qa_database)} characters\")\n",
    "\n",
    "# Let's see what we're working with\n",
    "qa_pairs = []\n",
    "lines = qa_database.strip().split('\\n')\n",
    "current_q = None\n",
    "current_a = None\n",
    "\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if line.startswith('Q: '):\n",
    "        current_q = line[3:]  # Remove 'Q: '\n",
    "    elif line.startswith('A: '):\n",
    "        current_a = line[3:]  # Remove 'A: '\n",
    "        if current_q and current_a:\n",
    "            qa_pairs.append(f\"Q: {current_q}\\nA: {current_a}\")\n",
    "            current_q = None\n",
    "            current_a = None\n",
    "\n",
    "print(f\"Parsed {len(qa_pairs)} Q&A pairs\")\n",
    "print(f\"\\nFirst few Q&A pairs:\")\n",
    "for i, qa in enumerate(qa_pairs[:3]):\n",
    "    print(f\"\\n--- Q&A {i+1} ---\")\n",
    "    print(qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(texts: List[str]) -> List[List[float]]:\n",
    "    \"\"\"Get embeddings for a list of texts using OpenAI\"\"\"\n",
    "    response = client.embeddings.create(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        input=texts\n",
    "    )\n",
    "    return [embedding.embedding for embedding in response.data]\n",
    "\n",
    "# Get embeddings for our Q&A pairs (not chunks this time)\n",
    "embeddings = get_embeddings(qa_pairs)\n",
    "print(f\"Generated embeddings for {len(embeddings)} Q&A pairs\")\n",
    "print(f\"Each embedding has {len(embeddings[0])} dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Q&A pairs to ChromaDB with embeddings\n",
    "collection.add(\n",
    "    documents=qa_pairs,\n",
    "    embeddings=embeddings,\n",
    "    ids=[f\"qa_{i}\" for i in range(len(qa_pairs))],\n",
    "    metadatas=[{\"qa_index\": i} for i in range(len(qa_pairs))]\n",
    ")\n",
    "\n",
    "print(f\"Added {len(qa_pairs)} Q&A pairs to ChromaDB collection\")\n",
    "\n",
    "# Demonstrate the concept: we don't need all 20 Q&A pairs to answer one question\n",
    "print(f\"\\n💡 Key insight: We have {len(qa_pairs)} Q&A pairs, but we only need 2-3 relevant ones to answer most questions!\")\n",
    "print(\"This saves tokens, reduces costs, and improves response quality.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 5: Semantic Search Experiments\n",
    "\n",
    "Now let's experiment with semantic search to find relevant chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_knowledge(query: str, n_results: int = 3) -> Dict:\n",
    "    \"\"\"Search for relevant Q&A pairs using semantic similarity\"\"\"\n",
    "    # Get embedding for the query\n",
    "    query_embedding = get_embeddings([query])[0]\n",
    "    \n",
    "    # Search in ChromaDB\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=n_results\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test semantic search with our Q&A database\n",
    "test_queries = [\n",
    "    \"What programming languages does Framna use?\",\n",
    "    \"Company work environment and culture\",  \n",
    "    \"Geographic location and office setup\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n🔍 Query: {query}\")\n",
    "    results = search_knowledge(query, n_results=2)\n",
    "    \n",
    "    for i, (doc, distance) in enumerate(zip(results['documents'][0], results['distances'][0])):\n",
    "        print(f\"\\n📋 Most relevant Q&A {i+1} (distance: {distance:.3f}):\")\n",
    "        print(f\"{doc}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 6: RAG System - Putting It All Together\n",
    "\n",
    "Now let's build a complete RAG system that retrieves relevant chunks and uses them as context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query(question: str, n_results: int = 2) -> str:\n",
    "    \"\"\"Complete RAG pipeline: retrieve relevant Q&A pairs and generate answer\"\"\"\n",
    "    \n",
    "    # Step 1: Retrieve relevant Q&A pairs\n",
    "    search_results = search_knowledge(question, n_results=n_results)\n",
    "    relevant_qas = search_results['documents'][0]\n",
    "    \n",
    "    # Step 2: Combine Q&A pairs into context\n",
    "    context = \"\\n\\n\".join(relevant_qas)\n",
    "    \n",
    "    # Step 3: Generate answer using context\n",
    "    prompt_template = \"\"\"Based on the following Q&A pairs about Framna, please answer the question. Use the information from the Q&A pairs to provide a comprehensive answer.\n",
    "\n",
    "Relevant Q&A pairs:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    formatted_prompt = prompt_template.format(\n",
    "        context=context,\n",
    "        question=question\n",
    "    )\n",
    "    \n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-4.1\",\n",
    "        input=[\n",
    "            {\"role\": \"user\", \"content\": formatted_prompt}\n",
    "        ],\n",
    "        max_tokens=300\n",
    "    )\n",
    "    \n",
    "    return response.output[0].content[0].text, relevant_qas\n",
    "\n",
    "# Test the RAG system with Q&A database\n",
    "test_questions = [\n",
    "    \"What technology stack does Framna use for mobile development?\",\n",
    "    \"How big is the Framna team?\",\n",
    "    \"What makes Framna environmentally conscious?\",\n",
    "    \"Where can Framna employees work from?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\"\\n❓ Question: {question}\")\n",
    "    answer, qas_used = rag_query(question)\n",
    "    \n",
    "    print(f\"\\n💡 Answer: {answer}\")\n",
    "    \n",
    "    print(f\"\\n📚 Q&A pairs used:\")\n",
    "    for i, qa in enumerate(qas_used):\n",
    "        print(f\"\\n  {i+1}. {qa}\")\n",
    "    \n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Context Injection**: We can provide external knowledge to LLMs through context\n",
    "2. **Scaling Problems**: Large documents/databases are expensive and inefficient to process entirely\n",
    "3. **RAG Solution**: Retrieve only relevant portions using semantic search\n",
    "4. **Q&A Database Pattern**: Perfect example of why RAG is needed - we had 20 Q&A pairs but only needed 2-3 for any given question\n",
    "5. **Embeddings**: Vector representations enable semantic similarity search beyond keyword matching\n",
    "6. **Efficiency**: RAG dramatically reduces token costs while maintaining answer quality\n",
    "\n",
    "## Next Steps\n",
    "- Experiment with different retrieval strategies (top-k, similarity thresholds)\n",
    "- Try different embedding models (text-embedding-3-large vs small)\n",
    "- Add metadata filtering (categories, dates, etc.)\n",
    "- Implement re-ranking of retrieved results\n",
    "- Add evaluation metrics for RAG quality (relevance, completeness)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
