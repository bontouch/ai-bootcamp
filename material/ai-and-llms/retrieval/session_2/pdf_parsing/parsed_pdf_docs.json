[{"filename": "rag-deck.pdf", "text": "RAG\nTechnique\n\nFebruary 2024\n\n\fOverview\n\nRetrieval-Augmented Generation \nenhances the capabilities of language \nmodels by combining them with a \nretrieval system. This allows the model \nto leverage external knowledge sources \nto generate more accurate and \ncontextually relevant responses.\n\nExample use cases\n\n- Provide answers with up-to-date \n\ninformation\n\n- Generate contextual responses\n\nWhat we\u2019ll cover\n\n\u25cf Technical patterns\n\n\u25cf Best practices\n\n\u25cf Common pitfalls\n\n\u25cf Resources\n\n3\n\n\fWhat is RAG\n\nRetrieve information to Augment the model\u2019s knowledge and Generate the output\n\n\u201cWhat is your \nreturn policy?\u201d\n\nask\n\nresult\n\nsearch\n\nLLM\n\nreturn information\n\nTotal refunds: 0-14 days\n50% of value vouchers: 14-30 days\n$5 discount on next order: > 30 days\n\n\u201cYou can get a full refund up \nto 14 days after the \npurchase, then up to 30 days \nyou would get a voucher for \nhalf the value of your order\u201d\n\nKnowledge \nBase / External \nsources\n\n4\n\n\fWhen to use RAG\n\nGood for  \u2705\n\nNot good for  \u274c\n\n\u25cf\n\n\u25cf\n\nIntroducing new information to the model \n\n\u25cf\n\nTeaching the model a speci\ufb01c format, style, \n\nto update its knowledge\n\nReducing hallucinations by controlling \n\ncontent\n\n/!\\ Hallucinations can still happen with RAG\n\nor language\n\u2794 Use \ufb01ne-tuning or custom models instead\n\n\u25cf\n\nReducing token usage\n\u2794 Consider \ufb01ne-tuning depending on the use \n\ncase\n\n5\n\n\fTechnical patterns\n\nData preparation\n\nInput processing\n\nRetrieval\n\nAnswer Generation\n\n\u25cf Chunking\n\n\u25cf\n\n\u25cf\n\nEmbeddings\n\nAugmenting \ncontent\n\n\u25cf\n\nInput \naugmentation\n\n\u25cf NER\n\n\u25cf\n\nSearch\n\n\u25cf Context window\n\n\u25cf Multi-step \nretrieval\n\n\u25cf Optimisation\n\n\u25cf\n\nSafety checks\n\n\u25cf\n\nEmbeddings\n\n\u25cf Re-ranking\n\n6\n\n\fTechnical patterns\nData preparation\n\nchunk documents into multiple \npieces for easier consumption\n\ncontent\n\nembeddings\n\n0.983, 0.123, 0.289\u2026\n\n0.876, 0.145, 0.179\u2026\n\n0.983, 0.123, 0.289\u2026\n\nAugment content \nusing LLMs\n\nEx: parse text only, ask gpt-4 to rephrase & \nsummarize each part, generate bullet points\u2026\n\nBEST PRACTICES\n\nPre-process content for LLM \nconsumption: \nAdd summary, headers for each \npart, etc.\n+ curate relevant data sources\n\nKnowledge \nBase\n\nCOMMON PITFALLS\n\n\u2794 Having too much low-quality \n\ncontent\n\n\u2794 Having too large documents\n\n7\n\n\fTechnical patterns\nData preparation: chunking\n\nWhy chunking?\n\nIf your system doesn\u2019t require \nentire documents to provide \nrelevant answers, you can \nchunk them into multiple pieces \nfor easier consumption (reduced \ncost & latency).\n\nOther approaches: graphs or \nmap-reduce\n\nThings to consider\n\n\u25cf\n\nOverlap:\n\n\u25cb\n\n\u25cb\n\nShould chunks be independent or overlap one \nanother?\nIf they overlap, by how much?\n\n\u25cf\n\nSize of chunks: \n\n\u25cb What is the optimal chunk size for my use case?\n\u25cb\n\nDo I want to include a lot in the context window or \njust the minimum?\n\n\u25cf Where to chunk:\n\n\u25cb\n\n\u25cb\n\nShould I chunk every N tokens or use speci\ufb01c \nseparators? \nIs there a logical way to split the context that would \nhelp the retrieval process?\n\n\u25cf What to return:\n\n\u25cb\n\n\u25cb\n\nShould I return chunks across multiple documents \nor top chunks within the same doc?\nShould chunks be linked together with metadata to \nindicate common properties?\n\n8\n\n\fTechnical patterns\nData preparation: embeddings\n\nWhat to embed?\n\nDepending on your use case \nyou might not want just to \nembed the text in the \ndocuments but metadata as well \n- anything that will make it easier \nto surface this speci\ufb01c chunk or \ndocument when performing a \nsearch\n\nExamples\n\nEmbedding Q&A posts in a forum\nYou might want to embed the title of the posts, \nthe text of the original question and the content of \nthe top answers.\nAdditionally, if the posts are tagged by topic or \nwith keywords, you can embed those too.\n\nEmbedding product specs\nIn additional to embedding the text contained in \ndocuments describing the products, you might \nwant to add metadata that you have on the \nproduct such as the color, size, etc. in your \nembeddings.\n\n9\n\n\fTechnical patterns\nData preparation: augmenting content\n\nWhat does \u201cAugmenting \ncontent\u201d mean?\n\nAugmenting content refers to \nmodi\ufb01cations of the original content \nto make it more digestible for a \nsystem relying on RAG. The \nmodi\ufb01cations could be a change in \nformat, wording, or adding \ndescriptive content such as \nsummaries or keywords.\n\nExample approaches\n\nMake it a guide*\nReformat the content to look more like \na step-by-step guide with clear \nheadings and bullet-points, as this \nformat is more easily understandable \nby an LLM.\n\nAdd descriptive metadata*\nConsider adding keywords or text that \nusers might search for when thinking \nof a speci\ufb01c product or service.\n\nMultimodality\nLeverage models \nsuch as Whisper or \nGPT-4V to \ntransform audio or \nvisual content into \ntext.\nFor example, you \ncan use GPT-4V to \ngenerate tags for \nimages or to \ndescribe slides.\n\n* GPT-4 can do this for you with the right prompt\n\n10\n\n\fTechnical patterns\nInput processing\n\nProcess input according to task\n\nQ&A\nHyDE:  Ask LLM to hypothetically answer the \nquestion & use the answer to search the KB\n\nembeddings\n\n0.983, 0.123, 0.289\u2026\n\n0.876, 0.145, 0.179\u2026\n\nContent search\nPrompt LLM to rephrase input & optionally add \nmore context\n\nquery\n\nSELECT * from items\u2026\n\nDB search\nNER:  Find relevant entities to be used for a \nkeyword search or to construct a search query\n\nkeywords\n\nred\n\nsummer\n\nBEST PRACTICES\n\nConsider how to transform the \ninput to match content in the \ndatabase\nConsider using metadata to \naugment the user input\n\nCOMMON PITFALLS\n\n\u2794 Comparing directly the input \nto the database without \nconsidering the task \nspeci\ufb01cities \n\n11\n\n\fTechnical patterns\nInput processing: input augmentation\n\nWhat is input augmentation?\n\nExample approaches\n\nAugmenting the input means turning \nit into something di\ufb00erent, either \nrephrasing it, splitting it in several \ninputs or expanding it.\nThis helps boost performance as \nthe LLM might understand better \nthe user intent.\n\nQuery \nexpansion*\nRephrase the \nquery to be \nmore \ndescriptive\n\nHyDE*\nHypothetically \nanswer the \nquestion & use \nthe answer to \nsearch the KB\n\nSplitting a query in N*\nWhen there is more than 1 question or \nintent in a user query, consider \nsplitting it in several queries\n\nFallback\nConsider \nimplementing a \n\ufb02ow where the LLM \ncan ask for \nclari\ufb01cation when \nthere is not enough \ninformation in the \noriginal user query \nto get a result\n(Especially relevant \nwith tool usage)\n\n* GPT-4 can do this for you with the right prompt\n\n12\n\n\fTechnical patterns\nInput processing: NER\n\nWhy use NER?\n\nUsing NER (Named Entity \nRecognition) allows to extract \nrelevant entities from the input, that \ncan then be used for more \ndeterministic search queries. \nThis can be useful when the scope \nis very constrained.\n\nExample\n\nSearching for movies\nIf you have a structured database containing \nmetadata on movies, you can extract genre, \nactors or directors names, etc. from the user \nquery and use this to search the database\n\nNote: You can use exact values or embeddings after \nhaving extracted the relevant entities\n\n13\n\n\fTechnical patterns\nRetrieval\n\nre-ranking\n\nINPUT\n\nembeddings\n\n0.983, 0.123, 0.289\u2026\n\n0.876, 0.145, 0.179\u2026\n\nquery\n\nSELECT * from items\u2026\n\nkeywords\n\nred\n\nsummer\n\nSemantic \nsearch\n\nRESULTS\n\nRESULTS\n\nvector DB\n\nrelational / \nnosql db\n\nFINAL RESULT\n\nUsed to \ngenerate output\n\nBEST PRACTICES\n\nUse a combination of semantic \nsearch and deterministic queries \nwhere possible\n\n+ Cache output where possible\n\nCOMMON PITFALLS\n\n\u2794 The wrong elements could be \ncompared when looking at \ntext similarity, that is why \nre-ranking is important\n\n14\n\n\fTechnical patterns\nRetrieval: search\n\nHow to search?\n\nSemantic search\n\nKeyword search\n\nSearch query\n\nThere are many di\ufb00erent \napproaches to search depending on \nthe use case and the existing \nsystem.\n\nUsing embeddings, you \ncan perform semantic \nsearches. You can \ncompare embeddings \nwith what is in your \ndatabase and \ufb01nd the \nmost similar.\n\nIf you have extracted \nspeci\ufb01c entities or \nkeywords to search for, \nyou can search for these \nin your database.\n\nBased on the extracted \nentities you have or the \nuser input as is, you can \nconstruct search queries \n(SQL, cypher\u2026) and use \nthese queries to search \nyour database.\n\nYou can use a hybrid approach and combine several of these.\nYou can perform multiple searches in parallel or in sequence, or \nsearch for keywords with their embeddings for example.\n\n15\n\n\fTechnical patterns\nRetrieval: multi-step retrieval\n\nWhat is multi-step retrieval?\n\nIn some cases, there might be \nseveral actions to be performed to \nget the required information to \ngenerate an answer.\n\nThings to consider\n\n\u25cf\n\nFramework to be used:\n\n\u25cb When there are multiple steps to perform, \nconsider whether you want to handle this \nyourself or use a framework to make it easier\n\n\u25cf\n\nCost & Latency:\n\n\u25cb\n\n\u25cb\n\nPerforming multiple steps at the retrieval \nstage can increase latency and cost \nsigni\ufb01cantly\nConsider performing actions in parallel to \nreduce latency\n\n\u25cf\n\nChain of Thought:\n\n\u25cb\n\n\u25cb\n\nGuide the assistant with the chain of thought \napproach: break down instructions into \nseveral steps, with clear guidelines on \nwhether to continue, stop or do something \nelse. \nThis is more appropriate when tasks need to \nbe performed sequentially - for example: \u201cif \nthis didn\u2019t work, then do this\u201d\n\n16\n\n\fTechnical patterns\nRetrieval: re-ranking\n\nWhat is re-ranking?\n\nExample approaches\n\nRe-ranking means re-ordering the \nresults of the retrieval process to \nsurface more relevant results.\nThis is particularly important when \ndoing semantic searches.\n\nRule-based re-ranking\nYou can use metadata to rank results by relevance. For \nexample, you can look at the recency of the documents, at \ntags, speci\ufb01c keywords in the title, etc.\n\nRe-ranking algorithms\nThere are several existing algorithms/approaches you can use \nbased on your use case: BERT-based re-rankers, \ncross-encoder re-ranking, TF-IDF algorithms\u2026\n\n17\n\n\fTechnical patterns\nAnswer Generation\n\nFINAL RESULT\n\nPiece of content \nretrieved\n\nLLM\n\nPrompt including \nthe content\n\nUser sees the \n\ufb01nal result\n\nBEST PRACTICES\n\nEvaluate performance after each \nexperimentation to assess if it\u2019s \nworth exploring other paths\n+ Implement guardrails if applicable\n\nCOMMON PITFALLS\n\n\u2794 Going for \ufb01ne-tuning without \ntrying other approaches\n\u2794 Not paying attention to the \nway the model is prompted\n\n18\n\n\fTechnical patterns\nAnswer Generation: context window\n\nHow to manage context?\n\nDepending on your use case, there are \nseveral things to consider when \nincluding retrieved content into the \ncontext window to generate an answer. \n\nThings to consider\n\n\u25cf\n\nContext window max size:\n\n\u25cb\n\n\u25cb\n\nThere is a maximum size, so putting too \nmuch content is not ideal\nIn conversation use cases, the \nconversation will be part of the context \nas well and will add to that size\n\n\u25cf\n\nCost & Latency vs Accuracy:\n\n\u25cb More context results in increased \n\nlatency and additional costs since there \nwill be more input tokens\nLess context might also result in \ndecreased accuracy\n\n\u25cb\n\n\u25cf\n\n\u201cLost in the middle\u201d problem:\n\n\u25cb When there is too much context, LLMs \ntend to forget the text \u201cin the middle\u201d of \nthe content and might look over some \nimportant information.\n\n19\n\n\fTechnical patterns\nAnswer Generation: optimisation\n\nHow to optimise?\n\nThere are a few di\ufb00erent \nmethods to consider when \noptimising a RAG application.\nTry them from left to right, and \niterate with several of these \napproaches if needed.\n\nPrompt Engineering\n\nFew-shot examples\n\nFine-tuning\n\nAt each point of the \nprocess, experiment with \ndi\ufb00erent prompts to get \nthe expected input format \nor generate a relevant \noutput.\nTry guiding the model if \nthe process to get to the \n\ufb01nal outcome contains \nseveral steps.\n\nIf the model doesn\u2019t \nbehave as expected, \nprovide examples of what \nyou want e.g. provide \nexample user inputs and \nthe expected processing \nformat.\n\nIf giving a few examples \nisn\u2019t enough, consider \n\ufb01ne-tuning a model with \nmore examples for each \nstep of the process: you \ncan \ufb01ne-tune to get a \nspeci\ufb01c input processing \nor output format.\n\n20\n\n\fTechnical patterns\nAnswer Generation: safety checks\n\nWhy include safety checks?\n\nJust because you provide the model \nwith (supposedly) relevant context \ndoesn\u2019t mean the answer will \nsystematically be truthful or on-point.\nDepending on the use case, you \nmight want to double-check. \n\nExample evaluation framework: RAGAS\n\n21\n\n\f", "pages_description": ["Overview\n\nThis section introduces the concept of Retrieval-Augmented Generation, which is a method that improves the abilities of language models. It does this by combining these models with a retrieval system. A retrieval system is a tool that can search and pull in information from external sources, such as databases or the internet. By using this approach, the language model can access up-to-date and relevant information from outside its own training data. This helps the model produce more accurate and contextually appropriate responses.\n\nTwo example use cases are provided:\n- The system can answer questions using the most current information available.\n- It can generate responses that are tailored to the specific context of the question or conversation.\n\nThe section also outlines what will be covered in the discussion:\n- Technical patterns: The structures and methods used to implement retrieval-augmented generation.\n- Best practices: Recommended approaches to get the best results.\n- Common pitfalls: Typical mistakes or challenges to watch out for.\n- Resources: Additional materials or references for further learning.\n\nOverall, this introduction sets the stage for understanding how retrieval-augmented generation works, why it is useful, and what topics will be explored in more detail.", "What is RAG\n\nRAG stands for \"Retrieve information to Augment the model\u2019s knowledge and Generate the output.\" This concept is about enhancing the abilities of a language model by allowing it to access and use external information sources when answering questions.\n\nHere's how the process works, as illustrated:\n\n1. A user asks a question, such as \"What is your return policy?\"\n2. This question is sent to a large language model (LLM).\n3. The LLM recognizes that it may not have the most up-to-date or specific information, so it searches an external knowledge base or other sources for relevant data.\n4. The knowledge base contains detailed information, for example:\n   - Total refunds are available for returns made within 0-14 days.\n   - For returns made between 14-30 days, customers receive vouchers worth 50% of the order value.\n   - For returns after 30 days, customers get a $5 discount on their next order.\n5. The LLM retrieves this information and generates a clear, user-friendly response, such as: \"You can get a full refund up to 14 days after the purchase, then up to 30 days you would get a voucher for half the value of your order.\"\n6. This answer is then returned to the user.\n\nThe key idea is that RAG allows the language model to provide more accurate and current answers by pulling in information from trusted external sources, rather than relying solely on what it was trained on. This makes the model more useful for tasks that require up-to-date or specialized knowledge.", "When to use RAG\n\nThis content explains the appropriate and inappropriate uses of RAG, which stands for Retrieval-Augmented Generation\u2014a technique used in AI to improve how models access and use information.\n\n**Good for:**\n- **Introducing new information to the model to update its knowledge:** RAG is effective when you want to provide the model with new or updated information that it didn't originally know. This helps keep the model's responses current and relevant.\n- **Reducing hallucinations by controlling content:** RAG can help decrease the chances of the model making up information (a phenomenon known as \"hallucination\") by allowing you to control the content it uses to generate answers. However, it's important to note that hallucinations can still occur even when using RAG.\n\n**Not good for:**\n- **Teaching the model a specific format, style, or language:** If your goal is to have the model consistently use a particular writing style, format, or language, RAG is not the best tool. Instead, you should use techniques like fine-tuning or creating custom models.\n- **Reducing token usage:** If you want to minimize the number of tokens (units of text) the model processes, RAG may not be the most efficient approach. In such cases, fine-tuning the model might be a better solution, depending on your specific needs.\n\nIn summary, RAG is best used for updating a model's knowledge and managing the information it uses, but it is not suitable for teaching specific styles or reducing processing costs. For those goals, other methods like fine-tuning are recommended.", "Technical patterns\n\nThis content outlines key technical patterns commonly used in building systems that process and generate information, such as those found in artificial intelligence and data-driven applications. The patterns are divided into four main categories:\n\n1. **Data preparation**\n   - **Chunking**: This involves breaking down large pieces of data into smaller, more manageable parts. For example, a long document might be split into paragraphs or sentences.\n   - **Embeddings**: This refers to converting data (like words or sentences) into numerical representations that a computer can understand and process.\n   - **Augmenting content**: This means enhancing the original data by adding extra information or context, which can help improve the performance of downstream tasks.\n\n2. **Input processing**\n   - **Input augmentation**: This is the process of modifying or enriching the input data before it is used, such as by adding synonyms or related terms.\n   - **NER (Named Entity Recognition)**: This technique identifies and classifies key elements in the text, such as names of people, places, or organizations.\n   - **Embeddings**: As in data preparation, embeddings are used here to represent the input data in a way that is suitable for further processing.\n\n3. **Retrieval**\n   - **Search**: This involves finding relevant information from a large collection of data based on a query or question.\n   - **Multi-step retrieval**: This is a more advanced search process that involves several stages, such as filtering and refining results to improve accuracy.\n   - **Re-ranking**: After retrieving initial results, this step involves sorting or prioritizing them to ensure the most relevant information is presented first.\n\n4. **Answer Generation**\n   - **Context window**: This refers to the amount of information the system considers at one time when generating an answer, which can affect the quality and relevance of the response.\n   - **Optimisation**: This involves improving the system\u2019s performance, such as making answers more accurate or efficient.\n   - **Safety checks**: These are measures put in place to ensure that the generated answers are appropriate, safe, and do not contain harmful or incorrect information.\n\nTogether, these technical patterns form the backbone of many modern information processing and AI systems, ensuring that data is handled efficiently and that the answers generated are both relevant and safe.", "Technical patterns: Data preparation\n\nThis content explains how to prepare data for use with large language models (LLMs), focusing on making information easier for these models to process and understand.\n\nThe process begins with the original content, such as documents. These documents are broken down into smaller pieces, a step called \"chunking.\" Chunking makes it easier for LLMs to handle and analyze the information.\n\nOnce the content is chunked, each piece is converted into a set of numbers called \"embeddings.\" Embeddings are a way to represent the meaning of text in a format that computers can work with. These embeddings are then stored in a knowledge base, which acts as a searchable database for the LLM.\n\nThere is also an option to enhance or \"augment\" the content using LLMs themselves. For example, you can use a model like GPT-4 to rephrase and summarize each chunk, or to generate bullet points. This step helps make the information even more accessible and useful for future queries.\n\nBest practices for this process include:\n- Pre-processing content specifically for LLM consumption. This means adding summaries and headers to each part of the content, making it easier for the model to understand and retrieve relevant information.\n- Carefully curating the data sources to ensure only relevant and high-quality information is included.\n\nCommon pitfalls to avoid are:\n- Including too much low-quality content, which can reduce the effectiveness of the knowledge base.\n- Using documents that are too large, which can make it difficult for the LLM to process and retrieve information efficiently.\n\nIn summary, effective data preparation for LLMs involves breaking down content, converting it into embeddings, possibly enhancing it with LLMs, and following best practices to ensure high quality and relevance. Avoiding common mistakes like low-quality or overly large documents is crucial for building a useful knowledge base.", "Technical patterns  \nData preparation: chunking\n\nThis content discusses the concept of \"chunking\" in data preparation, particularly for systems that process documents to provide answers or insights.\n\n**Why chunking?**  \nChunking is useful when your system does not need to process entire documents to generate relevant answers. Instead, you can break documents into smaller pieces, or \"chunks,\" which makes it easier and faster for the system to handle the data. This approach can reduce both the cost and the time it takes to get results (latency). Other methods for handling large data include using graphs or map-reduce techniques.\n\n**Things to consider when chunking:**\n\n- **Overlap:**  \n  - Should the chunks be completely separate, or should they share some content?  \n  - If they do overlap, how much overlap is appropriate?\n\n- **Size of chunks:**  \n  - What is the best chunk size for your specific use case?  \n  - Do you want to include a lot of information in each chunk, or just the minimum needed for context?\n\n- **Where to chunk:**  \n  - Should you split the document after a certain number of tokens (words or characters), or use specific separators like paragraphs or sentences?  \n  - Is there a logical way to divide the content that would make it easier to retrieve relevant information later?\n\n- **What to return:**  \n  - Should you return chunks from multiple documents, or just the top chunks from a single document?  \n  - Should you connect chunks with metadata to show that they share common properties or belong together?\n\nIn summary, chunking is a strategy to make large documents more manageable for automated systems, but it requires careful consideration of how to split and organize the data for the best results.", "Technical patterns  \nData preparation: embeddings\n\nThis content discusses how to prepare data for use with embeddings, which are a way to represent information (like text) in a format that computers can understand and search efficiently.\n\n**What to embed?**  \nThe main idea is that, depending on your specific needs, you might not want to embed only the main text from your documents. Instead, you can also include metadata\u2014extra information about the document or its parts. Metadata can be anything that helps make it easier to find or identify a specific piece of information when searching. For example, metadata could include the author, date, tags, or any other relevant details.\n\n**Examples:**\n\n- **Embedding Q&A posts in a forum:**  \n  When working with forum data, you might want to embed not just the main text of the posts, but also the title of each post, the original question, and the top answers. If the posts are organized by topics or keywords, you can embed those as well. This makes it easier to find relevant information when searching the forum.\n\n- **Embedding product specs:**  \n  For product information, you can embed the text that describes each product. Additionally, you can include metadata such as the product\u2019s color, size, or other specifications. Including this extra information in your embeddings helps improve search results and makes it easier to find products based on specific attributes.\n\nIn summary, when preparing data for embeddings, consider including both the main content and any useful metadata to improve search and retrieval. This approach helps ensure that searches return the most relevant results based on all available information.", "Technical patterns  \nData preparation: augmenting content\n\nThis section focuses on the concept of \"augmenting content\" as part of data preparation, especially for systems that use Retrieval-Augmented Generation (RAG).\n\n**What does \u201cAugmenting content\u201d mean?**  \nAugmenting content involves making changes to the original material to make it easier for a system to process and understand. This could mean changing the format, rewording, or adding extra information like summaries or keywords. The goal is to make the content more digestible for systems that rely on retrieving and generating information.\n\n**Example approaches:**\n\n1. **Make it a guide**  \n   One way to augment content is to reformat it into a step-by-step guide. This involves using clear headings and bullet points, making the information easier for a language model to understand and process. For example, instead of a long paragraph, you might break instructions into numbered steps or bullet points.\n\n2. **Add descriptive metadata**  \n   Another approach is to add keywords or descriptive text that users might search for. This helps the system quickly identify relevant content when a user is looking for information about a specific product or service.\n\n3. **Multimodality**  \n   This involves using advanced models like Whisper or GPT-4V to convert audio or visual content into text. For instance, you can use these models to generate tags for images or to describe the content of slides, making non-textual information accessible and searchable.\n\nA note is included that GPT-4 can perform these tasks if given the right prompt, highlighting the practical application of these techniques with current AI tools.\n\nIn summary, augmenting content is about making information easier for AI systems to use by reformatting, adding helpful descriptions, or converting different types of media into text. This improves the system\u2019s ability to retrieve and generate accurate, relevant responses.", "Technical patterns: Input processing\n\nThis content explains how to handle user input in technical systems, especially when working with large language models (LLMs) and databases. The main idea is to process the input according to the specific task you want to accomplish. Here\u2019s a breakdown of the key points:\n\n**Processing Input According to Task:**\n- **Q&A (Question and Answer):** \n  - Use a method called HyDE, where you ask the LLM to hypothetically answer the user\u2019s question. Then, use that answer to search the knowledge base (KB). This helps in finding more relevant information.\n- **Content Search:** \n  - Prompt the LLM to rephrase the user\u2019s input and, if needed, add more context. This makes the search more effective by aligning the input with how information is stored.\n- **DB (Database) Search:** \n  - Use Named Entity Recognition (NER) to find important entities (like names, places, or dates) in the input. These entities can then be used for keyword searches or to build a more precise search query.\n\n**Types of Output from Input Processing:**\n- **Embeddings:** \n  - These are numerical representations of the input, which can be used to find similar content in the database.\n- **Query:** \n  - This is a structured search command (like SQL: \u201cSELECT * from items\u2026\u201d) that retrieves data from the database.\n- **Keywords:** \n  - Important words or phrases (like \u201cred\u201d or \u201csummer\u201d) extracted from the input to help with searching.\n\n**Best Practices:**\n- Think about how to transform the user\u2019s input so it matches the way content is stored in the database.\n- Use metadata (extra information about the data) to enhance the user\u2019s input, making searches more accurate.\n\n**Common Pitfalls:**\n- A frequent mistake is to compare the user\u2019s input directly to the database without adjusting for the specific requirements of the task. This can lead to poor results because the input might not be in the right format or context for the database.\n\n**Summary:**  \nTo get the best results from input processing, always tailor the way you handle input to the specific task, use tools like LLMs and NER to improve search accuracy, and avoid simply matching raw input to database content. Consider transforming and enriching the input for more effective searches.", "Technical patterns  \nInput processing: input augmentation\n\nThis content focuses on the concept of input augmentation in the context of technical patterns, particularly for processing inputs in systems like large language models (LLMs).\n\n**What is input augmentation?**  \nInput augmentation involves transforming the original input into something different to improve how well a system, such as an LLM, understands it. This transformation can include rephrasing the input, splitting it into several parts, or expanding it with more detail. The main goal is to help the system better grasp the user's intent, which can lead to improved performance and more accurate responses.\n\n**Example approaches to input augmentation:**\n\n1. **Query expansion**  \n   This approach involves rephrasing the original query to make it more descriptive. By adding more detail or context, the system can better understand what the user is asking. For example, instead of asking \"weather,\" the query could be expanded to \"What is the weather forecast for New York City tomorrow?\"\n\n2. **HyDE (Hypothetical Document Embeddings)**  \n   In this method, the system first generates a hypothetical answer to the user's question and then uses that answer to search a knowledge base. This can help the system find more relevant information, even if the original query was vague or incomplete.\n\n3. **Splitting a query in N**  \n   When a user query contains multiple questions or intents, it can be helpful to split it into several separate queries. This allows the system to address each part individually, leading to more precise and useful answers.\n\n4. **Fallback**  \n   If the system does not have enough information from the original user query to provide a result, it can ask the user for clarification. This is especially important when using tools that require specific details to function correctly.\n\nA note is included that GPT-4 can perform these input augmentation techniques with the right prompt, highlighting the practical application of these methods in modern AI systems.\n\nIn summary, input augmentation is a set of strategies designed to make user inputs clearer and more actionable for AI systems, ultimately leading to better understanding and improved outcomes.", "Technical patterns  \nInput processing: NER\n\nThis content discusses the use of Named Entity Recognition (NER) as a technical pattern for processing input data.\n\n**Why use NER?**  \nNER, which stands for Named Entity Recognition, is a technique that helps extract important entities\u2014such as names, places, or specific terms\u2014from a given input. By identifying these entities, you can create more precise and reliable search queries. This approach is especially helpful when you are working within a very specific or limited scope, where accuracy is important.\n\n**Example: Searching for movies**  \nImagine you have a structured database that contains detailed information about movies, such as genres, actors, and directors. When a user submits a query, you can use NER to pull out relevant details like the genre or the names of actors and directors from their input. These extracted entities can then be used to search the database more effectively, ensuring that the results closely match what the user is looking for.\n\n**Additional Note:**  \nAfter extracting the relevant entities, you can use either the exact values or more advanced representations (called embeddings) to perform the search or further processing.\n\nIn summary, NER helps make search and data retrieval more accurate by focusing on the most important pieces of information in user input, which is particularly useful in well-defined or narrow domains.", "Technical patterns: Retrieval\n\nThis content explains how information retrieval works in technical systems, focusing on combining different search methods to get the best results.\n\nThe process starts with three types of input:\n- **Embeddings**: These are numerical representations of data, such as text, that capture their meaning. For example, a sentence might be turned into a list of numbers like 0.983, 0.123, 0.289, etc.\n- **Query**: This is a direct request for information, such as a database command (e.g., \"SELECT * from items...\").\n- **Keywords**: These are specific words or phrases, like \"red\" or \"summer,\" that help narrow down the search.\n\nThe system uses two main types of databases:\n- **Vector Database (vector DB)**: This is used for semantic search, which means it finds information based on meaning and context, not just exact words.\n- **Relational/NoSQL Database**: This is used for more traditional searches, where queries and keywords are matched exactly.\n\nThe retrieval process works as follows:\n1. Embeddings are sent to the vector database for semantic search, which returns a set of results.\n2. Queries and keywords are sent to the relational or NoSQL database, which also returns results.\n3. The results from the vector database can be re-ranked, meaning they are sorted or filtered again to improve accuracy.\n4. The final results from both databases are combined and used to generate the output.\n\nThere are also some important tips and warnings:\n- **Best Practices**: It's recommended to use both semantic search and deterministic (exact) queries together for better results. Also, caching (saving) the output when possible can improve performance.\n- **Common Pitfalls**: A common mistake is comparing the wrong elements when checking for text similarity. This is why re-ranking the results is important\u2014to make sure the most relevant information is selected.\n\nIn summary, this approach to retrieval combines the strengths of both semantic and traditional search methods, and emphasizes the importance of careful result handling to ensure accuracy.", "Technical patterns  \nRetrieval: search\n\nThis content explores different technical patterns for searching and retrieving information from a database or system. The main question addressed is: \"How to search?\" The answer depends on the specific use case and the system in place. There are several approaches to searching, each with its own strengths:\n\n1. **Semantic search**:  \n   This method uses \"embeddings,\" which are mathematical representations of words or phrases. By comparing these embeddings, the system can find items in the database that are most similar in meaning to the search input, rather than just matching exact words. This is useful for understanding the intent behind a search, even if the exact words don't match.\n\n2. **Keyword search**:  \n   In this approach, the system looks for specific words or entities that have been extracted from the user's input. It then searches the database for these keywords. This is a more traditional search method, focusing on exact matches of words or phrases.\n\n3. **Search query**:  \n   Here, the system constructs a formal search query (such as SQL or Cypher) based on the extracted entities or the user's input. This query is then used to search the database directly. This method is flexible and can be tailored to the structure of the database.\n\nThe content also highlights that these approaches are not mutually exclusive. You can use a hybrid approach, combining several methods. For example, you might perform multiple searches at the same time (in parallel) or one after another (in sequence). You could also search for keywords and then use their embeddings to enhance the search results.\n\nIn summary, the choice of search method depends on the needs of the application and the capabilities of the existing system. Combining different approaches can often yield the best results.", "Technical patterns  \nRetrieval: multi-step retrieval\n\nThe concept being discussed is multi-step retrieval, which refers to situations where more than one action is needed to gather all the necessary information to answer a question or solve a problem. Instead of retrieving information in a single step, the process involves several steps, each potentially dependent on the results of the previous one.\n\nKey points to consider when implementing multi-step retrieval:\n\n- **Framework to be used:**  \n  When multiple steps are required, you need to decide whether to manage these steps manually or use a framework that can simplify the process. Frameworks can help organize and automate the sequence of actions, making the process more efficient.\n\n- **Cost & Latency:**  \n  Performing several actions during the retrieval stage can increase both the time it takes to get results (latency) and the resources required (cost). To address this, you might consider running some actions in parallel, which can help reduce the overall time needed.\n\n- **Chain of Thought:**  \n  This approach involves guiding the system or assistant through a logical sequence of steps, breaking down the task into smaller, manageable parts. Clear instructions should be provided for each step, including when to continue, stop, or switch to a different action. This method is especially useful when tasks must be performed in a specific order, such as, \"If this step fails, then try the next one.\"\n\nIn summary, multi-step retrieval is about performing a series of actions to gather information, and it requires careful planning regarding frameworks, efficiency, and logical sequencing to ensure effective and timely results.", "Technical patterns  \nRetrieval: re-ranking\n\nRe-ranking is the process of re-ordering the results that come from a retrieval system, such as a search engine, to make sure the most relevant results appear at the top. This step is especially important when performing semantic searches, which try to understand the meaning behind a query rather than just matching keywords.\n\nThere are two main example approaches to re-ranking:\n\n1. **Rule-based re-ranking**: This method uses specific rules or metadata to decide how to rank the results. For instance, you might prioritize documents that are more recent, contain certain tags, or have specific keywords in their titles. These rules help ensure that the most useful or timely information appears first.\n\n2. **Re-ranking algorithms**: There are various algorithms designed to improve the ranking of search results. Some popular examples include:\n   - BERT-based re-rankers, which use advanced language models to better understand the context and meaning of queries and documents.\n   - Cross-encoder re-ranking, which evaluates the relationship between the query and each result more deeply.\n   - TF-IDF algorithms, which score documents based on how important certain words are within them compared to the whole collection.\n\nBy applying these approaches, systems can provide users with more accurate and helpful search results, improving the overall search experience.", "Technical patterns: Answer Generation\n\nThis content explains how answer generation works using large language models (LLMs), and highlights best practices and common pitfalls in the process.\n\nThe process begins with a piece of content that has been retrieved. This content is then included in a prompt that is sent to the LLM. The LLM processes the prompt, which contains the relevant information, and generates a final result. This result is then shown to the user.\n\n**Best Practices:**\n- After each experiment or change, evaluate the performance to determine if it\u2019s worthwhile to try different methods or approaches.\n- If necessary, implement guardrails\u2014these are safety measures or rules to ensure the model behaves as expected and avoids undesirable outputs.\n\n**Common Pitfalls:**\n- Jumping straight to fine-tuning the model without first exploring other, potentially simpler, approaches.\n- Not paying close attention to how the model is prompted. The way you structure the prompt can significantly affect the quality of the answers generated.\n\nIn summary, answer generation with LLMs involves retrieving relevant content, crafting an effective prompt, and then presenting the model\u2019s response to the user. Success depends on careful evaluation, thoughtful prompting, and considering alternatives before making major changes like fine-tuning.", "Technical patterns  \nAnswer Generation: context window\n\nHow to manage context?\n\nWhen generating answers using large language models, it's important to carefully manage the context window\u2014the portion of text the model can \"see\" at one time. The way you handle context depends on your specific use case, but there are several key factors to keep in mind when deciding what content to include in the context window:\n\nThings to consider\n\n- **Context window max size:**  \n  - There is a limit to how much content can fit in the context window. Adding too much information is not ideal because it can exceed this maximum size.\n  - In conversational applications, the ongoing conversation itself takes up part of the context window, further limiting how much additional information can be included.\n\n- **Cost & Latency vs Accuracy:**  \n  - Including more context can increase the time it takes to generate an answer (latency) and can also raise costs, since more input tokens are processed.\n  - On the other hand, using less context might make the answer less accurate, as the model has less information to work with.\n\n- **\"Lost in the middle\" problem:**  \n  - If the context window contains too much information, language models may overlook or \"forget\" important details that are located in the middle of the content. This means that some crucial information might not be considered when generating an answer.\n\nIn summary, managing the context window is a balancing act between including enough information for accuracy, staying within size limits, controlling costs and latency, and ensuring that no important details are overlooked.", "Technical patterns  \nAnswer Generation: optimisation\n\nThis content discusses different strategies for optimizing a Retrieval-Augmented Generation (RAG) application, which is a type of AI system that combines information retrieval with text generation. The main question addressed is: \"How to optimise?\"\n\nThere are several methods to consider, and it is recommended to try them in sequence from left to right, and to iterate with multiple approaches if necessary. The three main methods described are:\n\n1. **Prompt Engineering**:  \n   - This involves experimenting with different prompts at each stage of the process to achieve the desired input format or to generate relevant output.\n   - If the process to reach the final outcome involves multiple steps, you can guide the model through each step using carefully crafted prompts.\n\n2. **Few-shot examples**:  \n   - If the model does not behave as expected, you can provide it with examples of the desired outcome.\n   - For instance, you might give sample user inputs along with the expected processing format, helping the model understand what you want it to do.\n\n3. **Fine-tuning**:  \n   - If providing a few examples is not sufficient, you can fine-tune the model by training it with more examples for each step of the process.\n   - Fine-tuning allows you to achieve a specific input processing or output format by customizing the model\u2019s behavior more deeply.\n\nIn summary, optimizing a RAG application involves starting with prompt engineering, moving to providing few-shot examples if needed, and finally considering fine-tuning the model for more complex or specific requirements. These methods can be used individually or in combination to achieve the best results.", "Technical patterns  \nAnswer Generation: safety checks\n\nThis content discusses the importance of including safety checks when generating answers using AI models. The main question addressed is: \"Why include safety checks?\" The explanation provided is that even if you give the model content that you believe is relevant, it does not guarantee that the answer will always be truthful or accurate. The model might still produce answers that are incorrect or not directly related to the question. Therefore, depending on how critical the use case is, it may be necessary to double-check the answers generated by the model.\n\nTo help evaluate the quality and safety of generated answers, an example evaluation framework called \"RAGAS\" is introduced. This framework uses a scoring system, called the \"ragas score,\" which is divided into two main areas: generation and retrieval.\n\n- Under \"generation,\" two key aspects are measured:\n  - **Faithfulness**: This checks how factually accurate the generated answer is.\n  - **Answer relevancy**: This measures how relevant the generated answer is to the original question.\n\n- Under \"retrieval,\" two other aspects are considered:\n  - **Context precision**: This looks at the quality of the retrieved context, specifically the ratio of useful information to irrelevant information (signal to noise).\n  - **Context recall**: This checks whether all the necessary information required to answer the question has been retrieved.\n\nIn summary, safety checks are important because AI-generated answers are not always reliable, and frameworks like RAGAS help systematically evaluate and improve the quality and trustworthiness of these answers."]}, {"filename": "models-page.pdf", "text": "26/02/2024, 17:58\n\nModels - OpenAI API\n\nDocumentation\n\nAPI reference\n\nForum \n\nHelp \n\nModels\n\nOverview\n\nThe OpenAI API is powered by a diverse set of models with different capabilities and\nprice points. You can also make customizations to our models for your specific use\n\ncase with fine-tuning.\n\nMODEL\n\nDE S CRIPTION\n\nGPT-4 and GPT-4 Turbo A set of models that improve on GPT-3.5 and can\n\nunderstand as well as generate natural language or code\n\nGPT-3.5 Turbo\n\nA set of models that improve on GPT-3.5 and can\n\nunderstand as well as generate natural language or code\n\nDALL\u00b7E\n\nA model that can generate and edit images given a natural\n\nlanguage prompt\n\nTTS\n\nA set of models that can convert text into natural sounding\n\nspoken audio\n\nWhisper\n\nA model that can convert audio into text\n\nEmbeddings\n\nA set of models that can convert text into a numerical form\n\nModeration\n\nA fine-tuned model that can detect whether text may be\n\nsensitive or unsafe\n\nGPT base\n\nDeprecated\n\nA set of models without instruction following that can\nunderstand as well as generate natural language or code\n\nA full list of models that have been deprecated along with\nthe suggested replacement\n\nWe have also published open source models including Point-E, Whisper, Jukebox, and\nCLIP.\n\nContinuous model upgrades\n\nhttps://platform.openai.com/docs/models/overview\n\n1/10\n\n\f26/02/2024, 17:58\n\nModels - OpenAI API\n\ngpt-3.5-turbo ,  gpt-4 , and  gpt-4-turbo-preview  point to the latest model\nversion. You can verify this by looking at the response object after sending a request.\nThe response will include the specific model version used (e.g.  gpt-3.5-turbo-\n0613 ).\n\nWe also offer static model versions that developers can continue using for at least\nthree months after an updated model has been introduced. With the new cadence of\nmodel updates, we are also giving people the ability to contribute evals to help us\n\nimprove the model for different use cases. If you are interested, check out the OpenAI\nEvals repository.\n\nLearn more about model deprecation on our deprecation page.\n\nGPT-4 and GPT-4 Turbo\n\nGPT-4 is a large multimodal model (accepting text or image inputs and outputting text)\nthat can solve difficult problems with greater accuracy than any of our previous\n\nmodels, thanks to its broader general knowledge and advanced reasoning capabilities.\n\nGPT-4 is available in the OpenAI API to paying customers. Like  gpt-3.5-turbo , GPT-\n\n4 is optimized for chat but works well for traditional completions tasks using the Chat\nCompletions API. Learn how to use GPT-4 in our text generation guide.\n\nMODEL\n\nDE S CRIPTION\n\nCONTEXT\nWIND OW\n\nTRAINING\nDATA\n\ngpt-4-0125-preview\n\nNew  GPT-4 Turbo\n\n128,000\n\nUp to\n\nDec\n\n2023\n\nThe latest GPT-4 model\n\ntokens\n\nintended to reduce cases of\n\n\u201claziness\u201d where the model\ndoesn\u2019t complete a task.\nReturns a maximum of\n\n4,096 output tokens.\nLearn more.\n\ngpt-4-turbo-preview\n\nCurrently points to gpt-4-\n\n0125-preview.\n\ngpt-4-1106-preview\n\nGPT-4 Turbo model\nfeaturing improved\ninstruction following, JSON\n\nmode, reproducible outputs,\nparallel function calling, and\nmore. Returns a maximum\nof 4,096 output tokens. This\n\n128,000\ntokens\n\nUp to\nDec\n2023\n\n128,000\ntokens\n\nUp to\nApr 2023\n\nhttps://platform.openai.com/docs/models/overview\n\n2/10\n\n\f26/02/2024, 17:58\n\nModels - OpenAI API\n\nMODEL\n\nDE S CRIPTION\n\nis a preview model.\nLearn more.\n\nCONTEXT\nWIND OW\n\nTRAINING\nDATA\n\ngpt-4-vision-preview\n\nGPT-4 with the ability to\nunderstand images, in\n\n128,000\ntokens\n\nUp to\nApr 2023\n\naddition to all other GPT-4\nTurbo capabilities. Currently\npoints to gpt-4-1106-\n\nvision-preview.\n\ngpt-4-1106-vision-preview GPT-4 with the ability to\n\nunderstand images, in\naddition to all other GPT-4\n\nTurbo capabilities. Returns a\nmaximum of 4,096 output\n\ntokens. This is a preview\n\nmodel version. Learn more.\n\n128,000\ntokens\n\nUp to\nApr 2023\n\ngpt-4\n\ngpt-4-0613\n\nCurrently points to gpt-4-\n\n8,192\n\nUp to\n\n0613. See\n\ntokens\n\nSep 2021\n\ncontinuous model upgrades.\n\nSnapshot of gpt-4 from\n\nJune 13th 2023 with\n\nimproved function calling\n\nsupport.\n\n8,192\ntokens\n\nUp to\nSep 2021\n\ngpt-4-32k\n\nCurrently points to gpt-4-\n\ngpt-4-32k-0613\n\n32k-0613. See\n\ncontinuous model upgrades.\nThis model was never rolled\nout widely in favor of GPT-4\n\nTurbo.\n\nSnapshot of gpt-4-32k\n\nfrom June 13th 2023 with\nimproved function calling\nsupport. This model was\nnever rolled out widely in\n\nfavor of GPT-4 Turbo.\n\n32,768\n\ntokens\n\nUp to\n\nSep 2021\n\n32,768\n\ntokens\n\nUp to\n\nSep 2021\n\nFor many basic tasks, the difference between GPT-4 and GPT-3.5 models is not\nsignificant. However, in more complex reasoning situations, GPT-4 is much more\ncapable than any of our previous models.\n\nhttps://platform.openai.com/docs/models/overview\n\n3/10\n\n\f26/02/2024, 17:58\n\nModels - OpenAI API\n\nMultilingual capabilities\n\nGPT-4 outperforms both previous large language models and as of 2023, most state-\nof-the-art systems (which often have benchmark-specific training or hand-\nengineering). On the MMLU benchmark, an English-language suite of multiple-choice\nquestions covering 57 subjects, GPT-4 not only outperforms existing models by a\nconsiderable margin in English, but also demonstrates strong performance in other\nlanguages.\n\nGPT-3.5 Turbo\n\nGPT-3.5 Turbo models can understand and generate natural language or code and\nhave been optimized for chat using the Chat Completions API but work well for non-\nchat tasks as well.\n\nCONTEXT\nWIND OW\n\nTRAINING\nDATA\n\n16,385\n\ntokens\n\nUp to Sep\n\n2021\n\nMODEL\n\nDE S CRIPTION\n\ngpt-3.5-turbo-0125\n\nNew  Updated GPT 3.5 Turbo\n\nThe latest GPT-3.5 Turbo\nmodel with higher accuracy at\n\nresponding in requested\n\nformats and a fix for a bug\n\nwhich caused a text encoding\nissue for non-English\n\nlanguage function calls.\n\nReturns a maximum of 4,096\n\noutput tokens. Learn more.\n\ngpt-3.5-turbo\n\nCurrently points to gpt-3.5-\n\n4,096\n\nUp to Sep\n\nturbo-0613. The gpt-3.5-\n\ntokens\n\n2021\n\nturbo model alias will be\n\nautomatically upgraded from\ngpt-3.5-turbo-0613 to\n\ngpt-3.5-turbo-0125 on\n\nFebruary 16th.\n\ngpt-3.5-turbo-1106\n\nGPT-3.5 Turbo model with\nimproved instruction\n\n16,385\ntokens\n\nUp to Sep\n2021\n\nfollowing, JSON mode,\nreproducible outputs, parallel\nfunction calling, and more.\nReturns a maximum of 4,096\n\noutput tokens. Learn more.\n\nhttps://platform.openai.com/docs/models/overview\n\n4/10\n\n\f26/02/2024, 17:58\n\nModels - OpenAI API\n\nMODEL\n\nDE S CRIPTION\n\ngpt-3.5-turbo-instruct Similar capabilities as GPT-3\nera models. Compatible with\nlegacy Completions endpoint\nand not Chat Completions.\n\nCONTEXT\nWIND OW\n\nTRAINING\nDATA\n\n4,096\ntokens\n\nUp to Sep\n2021\n\ngpt-3.5-turbo-16k\n\nLegacy  Currently points to\ngpt-3.5-turbo-16k-0613.\n\n16,385\ntokens\n\nUp to Sep\n2021\n\ngpt-3.5-turbo-0613\n\nLegacy  Snapshot of gpt-3.5-\n\nturbo from June 13th 2023.\n\nWill be deprecated on June 13,\n2024.\n\n4,096\ntokens\n\nUp to Sep\n2021\n\ngpt-3.5-turbo-16k-0613\n\nLegacy  Snapshot of gpt-3.5-\n\n16,385\n\nUp to Sep\n\n16k-turbo from June 13th\n\ntokens\n\n2021\n\n2023. Will be deprecated on\n\nJune 13, 2024.\n\nDALL\u00b7E\n\nDALL\u00b7E is a AI system that can create realistic images and art from a description in\n\nnatural language. DALL\u00b7E 3 currently supports the ability, given a prompt, to create a\n\nnew image with a specific size. DALL\u00b7E 2 also support the ability to edit an existing\n\nimage, or create variations of a user provided image.\n\nDALL\u00b7E 3 is available through our Images API along with DALL\u00b7E 2. You can try DALL\u00b7E 3\n\nthrough ChatGPT Plus.\n\nMODEL\n\nDE S CRIPTION\n\ndall-e-3\n\nNew  DALL\u00b7E 3\n\nThe latest DALL\u00b7E model released in Nov 2023. Learn more.\n\ndall-e-2 The previous DALL\u00b7E model released in Nov 2022. The 2nd iteration of\nDALL\u00b7E with more realistic, accurate, and 4x greater resolution images\nthan the original model.\n\nTTS\n\nTTS is an AI model that converts text to natural sounding spoken text. We offer two\ndifferent model variates,  tts-1  is optimized for real time text to speech use cases\nand  tts-1-hd  is optimized for quality. These models can be used with the Speech\n\nendpoint in the Audio API.\n\nhttps://platform.openai.com/docs/models/overview\n\n5/10\n\n\f26/02/2024, 17:58\n\nModels - OpenAI API\n\nMODEL\n\nDE S CRIPTION\n\ntts-1\n\nNew  Text-to-speech 1\nThe latest text to speech model, optimized for speed.\n\ntts-1-hd\n\nNew  Text-to-speech 1 HD\nThe latest text to speech model, optimized for quality.\n\nWhisper\n\nWhisper is a general-purpose speech recognition model. It is trained on a large dataset\nof diverse audio and is also a multi-task model that can perform multilingual speech\nrecognition as well as speech translation and language identification. The Whisper v2-\n\nlarge model is currently available through our API with the  whisper-1  model name.\n\nCurrently, there is no difference between the open source version of Whisper and the\n\nversion available through our API. However, through our API, we offer an optimized\ninference process which makes running Whisper through our API much faster than\n\ndoing it through other means. For more technical details on Whisper, you can read the\n\npaper.\n\nEmbeddings\n\nEmbeddings are a numerical representation of text that can be used to measure the\n\nrelatedness between two pieces of text. Embeddings are useful for search, clustering,\n\nrecommendations, anomaly detection, and classification tasks. You can read more\nabout our latest embedding models in the announcement blog post.\n\nMODEL\n\nDE S CRIPTION\n\ntext-embedding-\n3-large\n\nNew  Embedding V3 large\nMost capable embedding model for both\n\nenglish and non-english tasks\n\ntext-embedding-\n\nNew  Embedding V3 small\n\n3-small\n\nIncreased performance over 2nd generation ada\nembedding model\n\ntext-embedding-\nada-002\n\nMost capable 2nd generation embedding\nmodel, replacing 16 first generation models\n\nOUTP UT\nDIMENSION\n\n3,072\n\n1,536\n\n1,536\n\nModeration\n\nhttps://platform.openai.com/docs/models/overview\n\n6/10\n\n\f26/02/2024, 17:58\n\nModels - OpenAI API\n\nThe Moderation models are designed to check whether content complies with\nOpenAI's usage policies. The models provide classification capabilities that look for\ncontent in the following categories: hate, hate/threatening, self-harm, sexual,\nsexual/minors, violence, and violence/graphic. You can find out more in our moderation\n\nguide.\n\nModeration models take in an arbitrary sized input that is automatically broken up into\nchunks of 4,096 tokens. In cases where the input is more than 32,768 tokens,\n\ntruncation is used which in a rare condition may omit a small number of tokens from\nthe moderation check.\n\nThe final results from each request to the moderation endpoint shows the maximum\n\nvalue on a per category basis. For example, if one chunk of 4K tokens had a category\nscore of 0.9901 and the other had a score of 0.1901, the results would show 0.9901 in the\nAPI response since it is higher.\n\nMODEL\n\nDE S CRIPTION\n\nMAX\nTOKENS\n\ntext-moderation-latest Currently points to text-moderation-\n\n32,768\n\n007.\n\ntext-moderation-stable Currently points to text-moderation-\n\n32,768\n\n007.\n\ntext-moderation-007\n\nMost capable moderation model across\nall categories.\n\n32,768\n\nGPT base\n\nGPT base models can understand and generate natural language or code but are not\ntrained with instruction following. These models are made to be replacements for our\n\noriginal GPT-3 base models and use the legacy Completions API. Most customers\n\nshould use GPT-3.5 or GPT-4.\n\nMODEL\n\nDE S CRIPTION\n\nbabbage-002 Replacement for the GPT-3 ada and\n\nbabbage base models.\n\ndavinci-002 Replacement for the GPT-3 curie and\n\ndavinci base models.\n\nMAX\nTOKENS\n\nTRAINING\nDATA\n\n16,384\ntokens\n\n16,384\ntokens\n\nUp to Sep\n2021\n\nUp to Sep\n2021\n\nHow we use your data\n\nhttps://platform.openai.com/docs/models/overview\n\n7/10\n\n\f26/02/2024, 17:58\n\nModels - OpenAI API\n\nYour data is your data.\n\nAs of March 1, 2023, data sent to the OpenAI API will not be used to train or improve\n\nOpenAI models (unless you explicitly opt in). One advantage to opting in is that the\nmodels may get better at your use case over time.\n\nTo help identify abuse, API data may be retained for up to 30 days, after which it will be\n\ndeleted (unless otherwise required by law). For trusted customers with sensitive\napplications, zero data retention may be available. With zero data retention, request\nand response bodies are not persisted to any logging mechanism and exist only in\nmemory in order to serve the request.\n\nNote that this data policy does not apply to OpenAI's non-API consumer services like\nChatGPT or DALL\u00b7E Labs.\n\nDefault usage policies by endpoint\n\nENDP OINT\n\nDATA USED\nFOR TRAINING\n\nDEFAULT\nRETENTION\n\nELIGIBLE FOR\nZERO RETENTION\n\n/v1/chat/completions*\n\nNo\n\n30 days\n\nYes, except\n\nimage inputs*\n\n/v1/files\n\n/v1/assistants\n\n/v1/threads\n\n/v1/threads/messages\n\n/v1/threads/runs\n\n/v1/threads/runs/steps\n\n/v1/images/generations\n\n/v1/images/edits\n\n/v1/images/variations\n\n/v1/embeddings\n\nNo\n\nNo\n\nNo\n\nNo\n\nNo\n\nNo\n\nNo\n\nNo\n\nNo\n\nNo\n\n/v1/audio/transcriptions No\n\nUntil deleted by\n\nNo\n\ncustomer\n\nUntil deleted by\n\nNo\n\ncustomer\n\n60 days *\n\n60 days *\n\n60 days *\n\n60 days *\n\n30 days\n\n30 days\n\n30 days\n\n30 days\n\nZero data\nretention\n\nNo\n\nNo\n\nNo\n\nNo\n\nNo\n\nNo\n\nNo\n\nYes\n\n-\n\nhttps://platform.openai.com/docs/models/overview\n\n8/10\n\n\f26/02/2024, 17:58\n\nModels - OpenAI API\n\nENDP OINT\n\nDATA USED\nFOR TRAINING\n\nDEFAULT\nRETENTION\n\nELIGIBLE FOR\nZERO RETENTION\n\n/v1/audio/translations\n\nNo\n\n/v1/audio/speech\n\n/v1/fine_tuning/jobs\n\n/v1/moderations\n\n/v1/completions\n\nNo\n\nNo\n\nNo\n\nNo\n\nZero data\nretention\n\n30 days\n\nUntil deleted by\ncustomer\n\nZero data\nretention\n\n-\n\nNo\n\nNo\n\n-\n\n30 days\n\nYes\n\n* Image inputs via the  gpt-4-vision-preview  model are not eligible for zero\nretention.\n\n* For the Assistants API, we are still evaluating the default retention period during the\n\nBeta. We expect that the default retention period will be stable after the end of the\n\nBeta.\n\nFor details, see our API data usage policies. To learn more about zero retention, get in\n\ntouch with our sales team.\n\nModel endpoint compatibility\n\nENDP OINT\n\nL ATE ST MODEL S\n\n/v1/assistants\n\nAll models except gpt-3.5-turbo-0301\n\nsupported. The retrieval tool requires gpt-4-\n\nturbo-preview (and subsequent dated model\n\nreleases) or gpt-3.5-turbo-1106 (and\n\nsubsequent versions).\n\n/v1/audio/transcriptions whisper-1\n\n/v1/audio/translations\n\nwhisper-1\n\n/v1/audio/speech\n\ntts-1, tts-1-hd\n\n/v1/chat/completions\n\ngpt-4 and dated model releases, gpt-4-turbo-\n\npreview and dated model releases, gpt-4-\n\nvision-preview, gpt-4-32k and dated model\n\nreleases, gpt-3.5-turbo and dated model\n\nhttps://platform.openai.com/docs/models/overview\n\n9/10\n\n\f26/02/2024, 17:58\n\nENDP OINT\n\nModels - OpenAI API\n\nL ATE ST MODEL S\n\nreleases, gpt-3.5-turbo-16k and dated model\n\nreleases, fine-tuned versions of gpt-3.5-turbo\n\n/v1/completions (Legacy) gpt-3.5-turbo-instruct, babbage-002,\n\ndavinci-002\n\n/v1/embeddings\n\ntext-embedding-3-small, text-embedding-\n\n3-large, text-embedding-ada-002\n\n/v1/fine_tuning/jobs\n\ngpt-3.5-turbo, babbage-002, davinci-002\n\n/v1/moderations\n\ntext-moderation-stable, text-\n\nhttps://platform.openai.com/docs/models/overview\n\n10/10\n\n\f", "pages_description": ["GPT-4 and GPT-4 Turbo\n\nThis section introduces GPT-4, a powerful AI model developed by OpenAI. GPT-4 is described as a large multimodal model, which means it can accept both text and image inputs and generate text as output. It is designed to solve complex problems with higher accuracy than previous models, thanks to its broader general knowledge and advanced reasoning abilities.\n\nGPT-4 is available to users of the OpenAI API who are paying customers. Like its predecessor, GPT-3.5-turbo, GPT-4 is optimized for chat applications but is also effective for traditional text completion tasks. Developers can use GPT-4 through the Chat Completions API, and there is a guide available for generating text with GPT-4.\n\nA table provides details about different versions of GPT-4 Turbo:\n\n- **gpt-4-0125-preview**: This is the latest version of GPT-4 Turbo. It is designed to reduce instances where the model fails to complete a task, a behavior referred to as \"laziness.\" It can return up to 4,096 output tokens and supports a context window of 128,000 tokens. The training data for this model goes up to December 2023.\n\n- **gpt-4-turbo-preview**: This version currently points to the gpt-4-0125-preview model, meaning it uses the same underlying technology and data. It also supports a context window of 128,000 tokens and is trained on data up to December 2023.\n\n- **gpt-4-1106-preview**: This is an earlier version of GPT-4 Turbo. It features improvements in following instructions, supports JSON mode (for structured data), reproducible outputs, and parallel function calling. Like the other versions, it can return up to 4,096 output tokens and supports a context window of 128,000 tokens. Its training data goes up to April 2023.\n\nIn summary, GPT-4 and its Turbo versions offer advanced capabilities for both chat and traditional text tasks, with improvements in accuracy, instruction following, and output reliability. The latest versions are trained on more recent data and are designed to be more effective in completing tasks.", "Models - OpenAI API\n\nThis content provides an overview of several versions of the GPT-4 model, highlighting their capabilities, context window sizes, and the timeframes of their training data.\n\n1. **gpt-4-vision-preview**: This version of GPT-4 can understand images in addition to all other GPT-4 Turbo capabilities. It currently points to the gpt-4-1106-vision-preview model. It supports a context window of 128,000 tokens and is trained on data up to April 2023.\n\n2. **gpt-4-1106-vision-preview**: Similar to the previous model, this version can also understand images and includes all GPT-4 Turbo capabilities. It can return a maximum of 4,096 output tokens and is considered a preview model. It also supports a context window of 128,000 tokens and is trained on data up to April 2023.\n\n3. **gpt-4**: This model currently points to gpt-4-0613 and supports a context window of 8,192 tokens. Its training data goes up to September 2021.\n\n4. **gpt-4-0613**: This is a snapshot of GPT-4 from June 13th, 2023, with improved function calling support. It also supports a context window of 8,192 tokens and is trained on data up to September 2021.\n\n5. **gpt-4-32k**: This model points to gpt-4-32k-0613 and supports a much larger context window of 32,768 tokens. However, it was never widely released in favor of GPT-4 Turbo. Its training data is up to September 2021.\n\n6. **gpt-4-32k-0613**: This is a snapshot of gpt-4-32k from June 13th, 2023, with improved function calling support. Like the previous model, it supports a context window of 32,768 tokens and was not widely released. Its training data is up to September 2021.\n\n**Key Insights:**\n- The main differences between these models are their ability to process images, the size of their context windows (how much information they can consider at once), and the recency of their training data.\n- The vision-preview models are notable for their ability to understand images, which is a significant advancement over text-only models.\n- Larger context windows allow the models to handle longer conversations or documents.\n- The note at the end emphasizes that for basic tasks, the difference between GPT-4 and GPT-3.5 models is minimal, but GPT-4 excels in more complex reasoning tasks, making it more capable overall.", "Multilingual capabilities\n\nGPT-4 is highlighted as outperforming previous large language models, especially as of 2023, when compared to other state-of-the-art systems. This is particularly evident on the MMLU benchmark, which is a set of multiple-choice questions in English covering 57 subjects. GPT-4 not only surpasses other models by a significant margin in English but also shows strong performance in other languages.\n\nGPT-3.5 Turbo\n\nGPT-3.5 Turbo models are designed to understand and generate both natural language and code. They are optimized for chat applications using the Chat Completions API but are also effective for tasks outside of chat.\n\nThree versions of GPT-3.5 Turbo are described:\n\n1. **gpt-3.5-turbo-0125**\n   - This is the newest version, called the \"Updated GPT 3.5 Turbo.\"\n   - It offers higher accuracy in responding to requested formats and fixes a bug related to text encoding for non-English language function calls.\n   - It can return up to 4,096 output tokens (a token is a piece of text, such as a word or part of a word).\n   - The context window, which is the amount of text the model can consider at once, is 16,385 tokens.\n   - The training data goes up to September 2021.\n\n2. **gpt-3.5-turbo**\n   - This is an alias that currently points to the version gpt-3.5-turbo-0613.\n   - The alias will automatically upgrade to gpt-3.5-turbo-0125 on February 16th.\n   - It has a context window of 4,096 tokens and uses training data up to September 2021.\n\n3. **gpt-3.5-turbo-1106**\n   - This version features improved instruction following, a JSON mode (for structured data), reproducible outputs, parallel function calling, and more.\n   - It also has a context window of 16,385 tokens and can return up to 4,096 output tokens.\n   - The training data is up to September 2021.\n\nIn summary, these models are continually updated to improve accuracy, handle more complex tasks, and support a wider range of languages and functions, making them versatile tools for both chat and non-chat applications.", "Models - OpenAI API\n\nThis content provides an overview of several AI models offered by OpenAI, focusing on language, image, and text-to-speech capabilities.\n\n**Language Models:**\n- **gpt-3.5-turbo-instruct:** This model has similar capabilities to the GPT-3 era models. It is compatible with older endpoints called \"Completions\" but not with the newer \"Chat Completions.\" It can handle up to 4,096 tokens (a token is a piece of text, like a word or part of a word) and was trained on data up to September 2021.\n- **gpt-3.5-turbo-16k:** This is a legacy model that currently points to another model called gpt-3.5-turbo-16k-0613. It can process up to 16,385 tokens and was also trained up to September 2021.\n- **gpt-3.5-turbo-0613:** This is a legacy snapshot of the gpt-3.5-turbo model from June 13, 2023. It will be deprecated (phased out) on June 13, 2024. It supports up to 4,096 tokens and was trained up to September 2021.\n- **gpt-3.5-turbo-16k-0613:** Another legacy snapshot, this time of the 16k version of gpt-3.5-turbo from June 13, 2023. It will also be deprecated on June 13, 2024. It supports up to 16,385 tokens and was trained up to September 2021.\n\n**DALL\u00b7E:**\nDALL\u00b7E is an AI system that generates realistic images and artwork from natural language descriptions. For example, you can describe a scene or object, and DALL\u00b7E will create an image based on that description.\n\n- **DALL\u00b7E 3:** The latest version, released in November 2023, can create new images from prompts and is available through the Images API and ChatGPT Plus. It offers improved capabilities over previous versions.\n- **DALL\u00b7E 2:** Released in November 2022, this version produces more realistic and accurate images with four times the resolution of the original DALL\u00b7E. It also allows editing existing images or creating variations of a user-provided image.\n\n**TTS (Text-to-Speech):**\nTTS is an AI model that converts written text into natural-sounding spoken language. There are two main versions:\n- **tts-1:** Optimized for real-time text-to-speech use cases, meaning it is fast and suitable for applications that need immediate audio output.\n- **tts-1-hd:** Optimized for higher quality, producing more natural and clear speech, though it may not be as fast as tts-1.\n\nBoth TTS models can be accessed through the Speech endpoint in the Audio API.\n\n**Summary:**\n- OpenAI offers a range of models for language processing, image generation, and text-to-speech.\n- The language models vary in their capabilities, context window (how much text they can handle at once), and training data.\n- DALL\u00b7E models create images from text, with the latest version offering the most advanced features.\n- TTS models turn text into speech, with options for speed or quality depending on the use case.", "Whisper\n\nWhisper is a versatile speech recognition model designed to handle a wide range of audio inputs. It is trained on a large and diverse dataset, which allows it to perform multiple tasks. These tasks include recognizing speech in different languages, translating speech, and identifying the language being spoken. The most recent and advanced version of this model, known as Whisper v2-large, is accessible through an API using the model name \"whisper-1\".\n\nThere is no functional difference between the open source version of Whisper and the version provided through the API. However, using the API offers a significant advantage: it uses an optimized process that makes running Whisper much faster compared to other methods. For those interested in the technical details or research behind Whisper, there are links provided to the open source code and the original research paper.\n\nEmbeddings\n\nEmbeddings are a way to represent text as numbers so that computers can measure how similar or related two pieces of text are. This is useful for a variety of tasks, such as searching for information, grouping similar items, making recommendations, detecting unusual patterns, and classifying data.\n\nThere are several embedding models available:\n\n- The \"Embedding V3 large\" model is the most advanced and capable, suitable for both English and non-English tasks. It produces an output with 3,072 dimensions, which means it represents each piece of text as a list of 3,072 numbers.\n- The \"Embedding V3 small\" model is a newer version that improves on the previous generation (called ada) and produces an output with 1,536 dimensions.\n- The \"text-embedding-ada-002\" model is the most capable of the second generation of embedding models, also producing 1,536 dimensions. It replaced 16 earlier models from the first generation.\n\nThese embedding models help computers understand and compare text in a way that is useful for many applications, from search engines to recommendation systems.", "Moderation Models and GPT Base Models\n\nThe Moderation models are designed to help determine whether content follows OpenAI's usage policies. These models can classify content into several categories, including hate, hate/threatening, self-harm, sexual, sexual/minors, violence, and violence/graphic. The purpose is to ensure that content shared or generated using OpenAI's tools is safe and appropriate.\n\nModeration models can handle inputs of any size by breaking them into chunks of 4,096 tokens each. If the input exceeds 32,768 tokens, some tokens may be omitted from the moderation check due to truncation, though this is rare. When the moderation endpoint processes a request, it returns the highest score for each category from all the chunks. For example, if one chunk scores 0.99 for a category and another scores 0.19, the result will show 0.99 for that category.\n\nA table lists the available moderation models:\n- **text-moderation-latest**: Currently points to text-moderation-007, with a maximum of 32,768 tokens.\n- **text-moderation-stable**: Also points to text-moderation-007, with the same token limit.\n- **text-moderation-007**: Described as the most capable moderation model across all categories, with a 32,768 token limit.\n\nGPT base models are also described. These models can understand and generate natural language or code, but they are not trained to follow instructions. They are intended as replacements for the original GPT-3 base models and use the older Completions API. Most users are encouraged to use GPT-3.5 or GPT-4 instead.\n\nA table lists the GPT base models:\n- **babbage-002**: Replaces the GPT-3 ada and babbage base models, with a maximum of 16,384 tokens and training data up to September 2021.\n- **davinci-002**: Replaces the GPT-3 curie and davinci base models, also with a 16,384 token limit and training data up to September 2021.\n\nThis information helps users understand the capabilities and limitations of the moderation and base language models provided by OpenAI, as well as which models are recommended for most use cases.", "Default usage policies by endpoint\n\nThis content explains how data sent to the OpenAI API is handled, focusing on privacy, data retention, and training policies.\n\n**Key Points:**\n- Your data is your own. As of March 1, 2023, data sent to the OpenAI API is not used to train or improve OpenAI models unless you specifically choose to opt in. If you do opt in, your use case may help improve the models over time.\n- To help prevent abuse, API data may be kept for up to 30 days before being deleted, unless the law requires otherwise. For customers with sensitive needs, there is an option for zero data retention, meaning the data is not stored at all and only exists in memory to process the request.\n- This policy does not apply to OpenAI\u2019s consumer services like ChatGPT or DALL\u00b7E Labs.\n\n**Table: Default usage policies by endpoint**\n\nThe table lists different API endpoints, whether data is used for training, how long data is kept by default, and if zero data retention is available.\n\n- **/v1/chat/completions**: Data is not used for training. Data is kept for 30 days by default. Zero retention is possible, except for image inputs.\n- **/v1/files**: Data is not used for training. Data is kept until the customer deletes it. Zero retention is not available.\n- **/v1/assistants**: Data is not used for training. Data is kept until the customer deletes it. Zero retention is not available.\n- **/v1/threads, /v1/threads/messages, /v1/threads/runs, /v1/threads/runs/steps**: Data is not used for training. Data is kept for 60 days by default. Zero retention is not available.\n- **/v1/images/generations, /v1/images/edits, /v1/images/variations**: Data is not used for training. Data is kept for 30 days by default. Zero retention is not available.\n- **/v1/embeddings**: Data is not used for training. Data is kept for 30 days by default. Zero retention is available.\n- **/v1/audio/transcriptions**: Data is not used for training. Zero data retention is the default.\n\n**Summary:**\nOpenAI\u2019s API is designed to protect user data by default, with options for stricter retention policies for sensitive use cases. Data is not used for training unless you opt in, and most data is deleted after a set period unless you delete it sooner or request zero retention. This approach helps balance user privacy with the need to prevent abuse and improve service quality.", "Model endpoint compatibility\n\nThe content provides information about data retention policies and model compatibility for various OpenAI API endpoints.\n\n**Data Retention and Training Policies:**\n\nA table lists several API endpoints, describing whether data is used for training, the default retention period, and eligibility for zero retention (meaning data is not stored at all):\n\n- **/v1/audio/translations**: Data is not used for training, and there is zero data retention.\n- **/v1/audio/speech**: Data is not used for training, with a default retention of 30 days. Zero retention is not available.\n- **/v1/fine_tuning/jobs**: Data is not used for training, and data is retained until deleted by the customer. Zero retention is not available.\n- **/v1/moderations**: Data is not used for training, and there is zero data retention.\n- **/v1/completions**: Data is not used for training, with a default retention of 30 days. This endpoint is eligible for zero retention.\n\nAdditional notes clarify that image inputs via the gpt-4-vision-preview model are not eligible for zero retention. For the Assistants API, the default retention period is still being evaluated during the Beta phase, but it is expected to stabilize after Beta ends.\n\n**Model Endpoint Compatibility:**\n\nAnother table outlines which models are compatible with specific API endpoints:\n\n- **/v1/assistants**: All models are supported except gpt-3.5-turbo-0301. The retrieval tool specifically requires gpt-4-turbo-preview (and later dated model releases) or gpt-3.5-turbo-1106 (and later versions).\n- **/v1/audio/transcriptions**: Compatible with the whisper-1 model.\n- **/v1/audio/translations**: Compatible with the whisper-1 model.\n- **/v1/audio/speech**: Compatible with tts-1 and tts-1-hd models.\n- **/v1/chat/completions**: Compatible with gpt-4 and its dated model releases, gpt-4-turbo-preview and its dated model releases, gpt-4-vision-preview, gpt-4-32k and its dated model releases, and gpt-3.5-turbo and its dated model releases.\n\n**Additional Information:**\n\n- For more details on data usage, users are directed to the API data usage policies.\n- For questions about zero retention, users are encouraged to contact the sales team.\n\n**Summary:**\n\nThis information helps users understand which models can be used with specific API endpoints and what the data retention policies are for each endpoint. It also highlights the importance of data privacy and provides guidance on where to find more information or get support.", "Models - OpenAI API\n\nThis content provides an overview of different API endpoints and the latest models available for each endpoint in the OpenAI API ecosystem. Here\u2019s a breakdown of the information:\n\n- The first column lists various API endpoints, which are specific paths you can use to interact with different functionalities of the OpenAI API.\n- The second column lists the latest models that are compatible with each endpoint.\n\n**Endpoints and Their Latest Models:**\n\n1. **/v1/completions (Legacy):**\n   - This endpoint is used for generating text completions.\n   - The latest models available for this endpoint are:\n     - gpt-3.5-turbo-instruct\n     - babbage-002\n     - davinci-002\n\n2. **/v1/embeddings:**\n   - This endpoint is used to generate vector representations (embeddings) of text, which are useful for tasks like semantic search or clustering.\n   - The latest models for embeddings are:\n     - text-embedding-3-small\n     - text-embedding-3-large\n     - text-embedding-ada-002\n\n3. **/v1/fine_tuning/jobs:**\n   - This endpoint is for managing fine-tuning jobs, which allow you to customize models for specific tasks.\n   - The latest models that can be fine-tuned are:\n     - gpt-3.5-turbo\n     - babbage-002\n     - davinci-002\n\n4. **/v1/moderations:**\n   - This endpoint is used for content moderation, helping to detect and filter harmful or inappropriate content.\n   - The latest models for moderation are:\n     - text-moderation-stable\n     - (The list appears to be cut off, but it starts with \"text-\")\n\n**Additional Notes:**\n- There is a mention of \"releases, gpt-3.5-turbo-16k and dated model releases, fine-tuned versions of gpt-3.5-turbo\" at the top, indicating that there are also special or extended versions of the gpt-3.5-turbo model, such as one with a larger context window (16k tokens).\n- Each endpoint is designed for a specific type of task, and the models listed are the most up-to-date options for those tasks.\n\n**Summary:**\nThis overview helps developers and users of the OpenAI API understand which models are available for different types of tasks, such as text generation, embeddings, fine-tuning, and moderation. By choosing the appropriate endpoint and model, users can optimize their applications for performance, accuracy, and the latest features."]}, {"filename": "evals-decks.pdf", "text": "Evaluation\nTechnique\n\nFebruary 2024\n\n\fOverview\n\nEvaluation is the process of validating \nand testing the outputs that your LLM \napplications are producing. Having \nstrong evaluations (\u201cevals\u201d) will mean a \nmore stable, reliable application which is \nresilient to code and model changes.\n\nExample use cases\n\n- Quantify a solution\u2019s reliability\n- Monitor application performance in \n\nproduction\nTest for regressions \n\n-\n\nWhat we\u2019ll cover\n\n\u25cf What are evals\n\n\u25cf Technical patterns\n\n\u25cf Example framework\n\n\u25cf Best practices\n\n\u25cf Resources\n\n3\n\n\fWhat are evals\nExample\n\nAn evaluation contains a question and a correct answer. We call this the ground truth.\n\nQuestion\n\nWhat is the population \nof Canada?\n\nThought: I don\u2019t know. I \nshould use a tool\nAction: Search\nAction Input: What is the \npopulation of Canada?\n\nLLM\n\nSearch\n\nThere are 39,566,248 people \nin Canada as of 2023.\n\nThe current population of \nCanada is 39,566,248 as of \nTuesday, May 23, 2023\u2026.\n\nActual result\n\n4\n\n\fWhat are evals\nExample\n\nOur ground truth matches the predicted answer, so the evaluation passes!\n\nEvaluation\n\nQuestion\n\nGround Truth\n\nPredicted Answer\n\nWhat is the population \nof Canada?\n\nThe population of Canada in \n2023 is 39,566,248 people.\n\nThere are 39,566,248 people \nin Canada as of 2023.\n\n5\n\n\fTechnical patterns\n\nMetric-based evaluations\n\nComponent evaluations\n\nSubjective evaluations\n\n\u25cf\n\n\u25cf\n\nComparison metrics like \nBLEU, ROUGE\n\nGives a score to \ufb01lter and \nrank results\n\n\u25cf\n\n\u25cf\n\nCompares ground \ntruth to prediction\n\nGives Pass/Fail\n\n\u25cf\n\n\u25cf\n\nUses a scorecard to \nevaluate subjectively\n\nScorecard may also \nhave a Pass/Fail\n\n6\n\n\fTechnical patterns\nMetric-based evaluations\n\nROUGE is a common metric for evaluating machine summarizations of text\n\nROUGE\n\nMetric for evaluating \nsummarization tasks\n\nOriginal\n\nOpenAI's mission is to ensure that \narti\ufb01cial general intelligence (AGI) \nbene\ufb01ts all of humanity. OpenAI \nwill build safe and bene\ufb01cial AGI \ndirectly, but will also consider its \nmission ful\ufb01lled if its work aids \nothers to achieve this outcome. \nOpenAI follows several key \nprinciples for this purpose. First, \nbroadly distributed bene\ufb01ts - any \nin\ufb02uence over AGI's deployment \nwill be used for the bene\ufb01t of all, \nand to avoid harmful uses or undue \nconcentration of power\u2026\n\nMachine \nSummary\n\nOpenAI aims to ensure AGI is \nfor everyone's use, totally \navoiding harmful stuff or big \npower concentration. \nCommitted to researching \nAGI's safe side, promoting \nthese studies in AI folks. \nOpenAI wants to be top in AI \nthings and works with \nworldwide research, policy \ngroups to \ufb01gure AGI's stuff.\n\nROUGE \nScore\n\n0.51162\n\n7\n\n\fTechnical patterns\nMetric-based evaluations\n\nBLEU score is another standard metric, this time focusing on machine translation tasks\n\nBLEU\n\nOriginal text\n\nReference\nTranslation\n\nPredicted \nTranslation\n\nMetric for \nevaluating \ntranslation tasks\n\nY gwir oedd \ndoedden nhw \nddim yn dweud \ncelwyddau wedi'r \ncwbl.\n\nThe truth was \nthey were not \ntelling lies after \nall.\n\nThe truth was \nthey weren't \ntelling lies after \nall.\n\nBLEU \nScore\n\n0.39938\n\n8\n\n\fTechnical patterns\nMetric-based evaluations\n\nWhat they\u2019re good for\n\nWhat to be aware of\n\n\u25cf\n\n\u25cf\n\nA good starting point for evaluating a \n\n\u25cf Not tuned to your speci\ufb01c context\n\nfresh solution\n\nUseful yardstick for automated testing \n\nof whether a change has triggered a \n\nmajor performance shift\n\n\u25cf Most customers require more \n\nsophisticated evaluations to go to \n\nproduction\n\n\u25cf Cheap and fast\n\n9\n\n\fTechnical patterns\nComponent evaluations\n\nComponent evaluations (or \u201cunit tests\u201d) cover a single input/output of the application. They check \nwhether each component works in isolation, comparing the input to a ground truth ideal result\n\nIs this the \ncorrect action?\n\nExact match \ncomparison\n\nDoes this answer \nuse the context?\n\nExtract numbers \nfrom each and \ncompare\n\nWhat is the population \nof Canada?\n\nThought: I don\u2019t know. I \nshould use a tool\nAction: Search\nAction Input: What is the \npopulation of Canada?\n\nAgent\n\nSearch\n\nThere are 39,566,248 people \nin Canada as of 2023.\n\nThe current population of \nCanada is 39,566,248 as of \nTuesday, May 23, 2023\u2026.\n\nIs this the right \nsearch result?\n\nTag the right \nanswer and do \nan exact match \ncomparison with \nthe retrieval.\n\n10\n\n\fTechnical patterns\nSubjective evaluations\n\nBuilding up a good scorecard for automated testing bene\ufb01ts from a few rounds of detailed human \nreview so we can learn what is valuable. \n\nA policy of \u201cshow rather than tell\u201d is also advised for GPT-4, so include examples of what a 1, 3 and \n8 out of 10 look like so the model can appreciate the spread.\n\nExample \nscorecard\n\nYou are a helpful evaluation assistant who grades how well the Assistant has answered the customer\u2019s query.\n\nYou will assess each submission against these metrics, please think through these step by step:\n\n-\n\nrelevance: Grade how relevant the search content is to the question from 1 to 5 // 5 being highly relevant and 1 being \nnot relevant at all.\n\n- credibility: Grade how credible the sources provided are from 1 to 5 // 5 being an established newspaper, \n\n-\n\ngovernment agency or large company and 1 being unreferenced.\nresult: Assess whether the question is correct given only the content returned from the search and the user\u2019s \nquestion // acceptable values are \u201ccorrect\u201d or \u201cincorrect\u201d\n\nYou will output this as a JSON document: {relevance: integer, credibility: integer, result: string}\n\nUser: What is the population of Canada?\nAssistant: Canada's population was estimated at 39,858,480 on April 1, 2023 by Statistics Canada.\nEvaluation: {relevance: 5, credibility: 5, result: correct}\n\n11\n\n\fExample framework\n\nYour evaluations can be grouped up into test suites called runs and executed in a batch to test \nthe e\ufb00ectiveness of your system.\n\nEach run should have its contents logged and stored at the most granular level possible \n(\u201ctracing\u201d) so you can investigate failure reasons, make tweaks and then rerun your evals.\n\nRun ID Model\n\nScore\n\nAnnotation feedback\n\nChanges since last run\n\n1\n\n2\n\n3\n\n4\n\n5\n\ngpt-3.5-turbo 28/50\n\ngpt-4\n\n36/50\n\ngpt-3.5-turbo 34/50\n\n\u25cf 18 incorrect with correct search results\n\u25cf 4 incorrect searches\n\nN/A\n\n\u25cf 10 incorrect with correct search results\n\u25cf 4 incorrect searches\n\n\u25cf 12 incorrect with correct search results\n\u25cf 4 incorrect searches\n\nModel updated to GPT-4\n\nAdded few-shot examples\n\ngpt-3.5-turbo 42/50\n\n\u25cf 8 incorrect with correct search results\n\nAdded metadata to search\nPrompt engineering for Answer step\n\ngpt-3.5-turbo 48/50\n\n\u25cf 2 incorrect with correct search results\n\nPrompt engineering to Answer step\n\n12\n\n\fExample framework\n\nI want to return a \nT-shirt I bought on \nAmazon on March 3rd.\n\nUser\n\nRouter\n\nLLM\n\nExpected: return\nPredicted: return\nPASS\n\nReturn\nAssistant\n\nLLM\n\nComponent evals\n\nSubjective evals\n\nExpected: return_policy\nPredicted: return_policy\nPASS\n\nKnowledge \nbase\n\nQuestion: Does this response adhere to \nour guidelines\nScore: \nPoliteness: 5, Coherence: 4, Relevancy: 4\nPASS\n\nSure - because we\u2019re \nwithin 14 days of the \npurchase, I can \nprocess the return\n\nQuestion: I want to return a T-shirt I \nbought on Amazon on March 3rd.\nGround truth: Eligible for return\nPASS\n\n13\n\n\fBest practices\n\nLog everything\n\n\u25cf\n\nEvals need test cases - log everything as you develop so you can mine your logs for good eval cases\n\nCreate a feedback loop\n\n\u25cf\n\u25cf\n\nBuild evals into your application so you can quickly run them, iterate and rerun to see the impact\nEvals also provide a useful structure for few-shot or \ufb01ne-tuning examples when optimizing\n\nEmploy expert labellers who know the process\n\n\u25cf Use experts to help create your eval cases - these need to be as lifelike as possible\n\nEvaluate early and often\n\n\u25cf\n\nEvals are something you should build as soon as you have your \ufb01rst functioning prompt - you won\u2019t be \nable to optimize without this baseline, so build it early\n\n\u25cf Making evals early also forces you to engage with what a good response looks like\n\n\f", "pages_description": ["Overview\n\nThis section introduces the concept of evaluation in the context of applications that use large language models (LLMs). Evaluation is described as the process of validating and testing the outputs generated by these applications. The main point is that having strong evaluations\u2014often referred to as \"evals\"\u2014leads to applications that are more stable and reliable. This stability makes the application more resilient to changes in the underlying code or the language model itself.\n\nSeveral example use cases for evaluation are provided:\n- Quantifying a solution\u2019s reliability: This means measuring how dependable the application is.\n- Monitoring application performance in production: This involves keeping track of how well the application works when it is being used by real users.\n- Testing for regressions: This refers to checking that new changes or updates do not break existing functionality or cause the application to perform worse.\n\nThe section also outlines the topics that will be covered:\n- What are evals: An explanation of what evaluations are and why they matter.\n- Technical patterns: Common methods or strategies used in evaluation.\n- Example framework: A practical example or structure for implementing evaluations.\n- Best practices: Recommendations for how to conduct evaluations effectively.\n- Resources: Additional materials or references for further learning.\n\nOverall, this introduction emphasizes the importance of evaluation in ensuring that LLM-based applications remain robust and effective as they evolve.", "What are evals  \nExample\n\nThis content explains the concept of \"evals,\" which are evaluations used to test the performance of a system, such as a language model. An evaluation consists of a question and a correct answer, which is referred to as the \"ground truth.\"\n\nThe example provided demonstrates how an evaluation works:\n\n- A person asks the question: \"What is the population of Canada?\"\n- This question is sent to a language model (LLM).\n- The LLM processes the question and thinks, \"I don\u2019t know. I should use a tool.\" It then decides to perform a search with the input: \"What is the population of Canada?\"\n- The search tool returns the answer: \"The current population of Canada is 39,566,248 as of Tuesday, May 23, 2023\u2026\"\n- The LLM then provides this answer.\n\nThe diagram highlights two key elements:\n- The \"Question\" is what the user asks.\n- The \"Actual result\" is the correct answer, which in this case is: \"There are 39,566,248 people in Canada as of 2023.\"\n\nThe process shows how the system is evaluated: the answer given by the LLM is compared to the ground truth to determine if it is correct. This helps measure how well the system can find and provide accurate information.", "What are evals\n\nThis content explains the concept of \"evals\" using a simple example. Evals, short for evaluations, are used to check if a predicted answer matches the correct or expected answer, known as the \"ground truth.\"\n\nThe example provided is a question-and-answer scenario:\n\n- The question is: \"What is the population of Canada?\"\n- The ground truth, or the correct answer, is: \"The population of Canada in 2023 is 39,566,248 people.\"\n- The predicted answer, which is the answer given by a system or model, is: \"There are 39,566,248 people in Canada as of 2023.\"\n\nThe evaluation process compares the ground truth with the predicted answer. In this case, both answers provide the same information, even though the wording is slightly different. Because the key information matches (the population number and the year), the evaluation is considered a pass, as indicated by a green check mark.\n\nThis example demonstrates that evals are a way to measure whether a system's answers are correct by comparing them to known, accurate information. If the answers match, the evaluation is successful. This process is important for testing and improving the accuracy of systems that provide information or make predictions.", "Technical patterns\n\nThis content explains three main approaches to evaluating technical systems, especially in the context of machine learning or artificial intelligence.\n\n1. Metric-based evaluations:\n- This approach uses quantitative metrics to compare system outputs. Examples of such metrics include BLEU and ROUGE, which are commonly used to assess the quality of text generated by models by comparing it to reference texts.\n- The main purpose of these metrics is to assign a numerical score to each result, which can then be used to filter out poor results and rank the remaining ones from best to worst.\n\n2. Component evaluations:\n- This method involves directly comparing the system's prediction to the correct answer, which is often called the \"ground truth.\"\n- The outcome is usually binary: each prediction is marked as either a \"Pass\" if it matches the ground truth, or a \"Fail\" if it does not.\n\n3. Subjective evaluations:\n- In this approach, a human evaluator uses a scorecard to judge the quality of the system's output based on personal or expert opinion, rather than strict numerical metrics.\n- The scorecard can include various criteria and may also result in a simple Pass/Fail judgment, depending on how it is designed.\n\nIn summary, these three patterns\u2014metric-based, component, and subjective evaluations\u2014offer different ways to assess the performance of technical systems, each with its own strengths and use cases. Metric-based evaluations provide objective scores, component evaluations offer clear-cut Pass/Fail results, and subjective evaluations allow for nuanced human judgment.", "Technical patterns  \nMetric-based evaluations\n\nThis content explains how metric-based evaluations are used to assess machine-generated summaries of text, focusing on a metric called ROUGE.\n\nROUGE is a widely used metric for evaluating the quality of text summarization tasks performed by machines. It works by comparing the machine-generated summary to a reference summary, often written by a human, to see how similar they are.\n\nThe example provided shows an original text about OpenAI's mission and principles. The original text explains that OpenAI's goal is to ensure artificial general intelligence (AGI) benefits all of humanity. It mentions building safe and beneficial AGI, considering the mission fulfilled if it helps others achieve this outcome, and following principles like broadly distributed benefits and avoiding harmful uses or undue concentration of power.\n\nNext, a machine-generated summary of this text is shown. The summary condenses the main points: OpenAI aims to ensure AGI is safe and available for everyone, avoids harmful uses and power concentration, promotes safe AI research, and collaborates with global research and policy groups.\n\nThe ROUGE score, which in this case is 0.51162, quantifies how closely the machine summary matches the original text. A higher ROUGE score indicates a closer match, meaning the summary captures more of the important content from the original.\n\nIn summary, ROUGE provides a way to objectively measure how well a machine can summarize text by comparing its output to a reference, helping researchers improve the quality of automated summarization systems.", "Technical patterns  \nMetric-based evaluations\n\nThis content explains how the BLEU score is used as a standard metric for evaluating machine translation tasks. BLEU stands for \"Bilingual Evaluation Understudy\" and is a way to measure how close a machine-generated translation is to a human reference translation.\n\nThe process starts with an original text in a foreign language. In this example, the original text is in Welsh: \"Y gwir oedd doedden nhw ddim yn dweud celwyddau wedi'r cwbl.\"\n\nNext, there is a reference translation, which is a human-produced translation of the original text. Here, the reference translation is: \"The truth was they were not telling lies after all.\"\n\nThen, there is a predicted translation, which is the output from a machine translation system. In this case, the predicted translation is: \"The truth was they weren't telling lies after all.\"\n\nThe BLEU score is then calculated by comparing the predicted translation to the reference translation. The closer the predicted translation is to the reference, the higher the BLEU score. In this example, the BLEU score is 0.39938, which suggests a moderate level of similarity between the machine translation and the human translation.\n\nIn summary, the BLEU score provides a quantitative way to evaluate how well a machine translation matches a human translation, helping researchers and developers improve translation systems.", "Technical patterns: Metric-based evaluations\n\nMetric-based evaluations are a common approach in technical fields for assessing the performance or effectiveness of a solution using quantitative measures.\n\n**What they\u2019re good for:**\n- They provide a good starting point for evaluating a new or fresh solution. This means that when you have something new to test, using metrics can quickly give you an idea of how well it works.\n- They serve as a useful yardstick for automated testing. This is especially helpful to check if a recent change has caused a significant shift in performance, making it easier to catch big issues early.\n- They are cheap and fast, meaning they don\u2019t require a lot of resources or time to implement, making them accessible for quick checks and early-stage evaluations.\n\n**What to be aware of:**\n- Metric-based evaluations are not tuned to your specific context. This means that while they provide general insights, they might not reflect the unique needs or challenges of your particular situation.\n- Most customers require more sophisticated evaluations before moving a solution into production. In other words, while metrics are a good first step, more detailed and tailored assessments are usually necessary to ensure a solution is truly ready for real-world use.\n\nIn summary, metric-based evaluations are a practical and efficient way to start assessing new solutions, but they have limitations and should be complemented with more in-depth evaluations for critical or production-level decisions.", "Technical patterns  \nComponent evaluations\n\nThis content explains how component evaluations, also known as \"unit tests,\" are used to assess individual parts of an application. The main goal is to check if each component works correctly on its own by comparing its output to a \"ground truth,\" which is the ideal or correct result.\n\nThe process is illustrated with an example involving a user asking, \"What is the population of Canada?\" Here\u2019s how the evaluation works:\n\n1. **User Input:** The user asks, \"What is the population of Canada?\"\n2. **Agent Response:** The agent receives the question and thinks, \"I don\u2019t know. I should use a tool.\" The agent then decides to perform a search with the input, \"What is the population of Canada?\"\n3. **Search Component:** The search tool retrieves information and returns, \"The current population of Canada is 39,566,248 as of Tuesday, May 23, 2023\u2026\"\n4. **Agent\u2019s Final Answer:** The agent uses this information to answer, \"There are 39,566,248 people in Canada as of 2023.\"\n\nThe diagram highlights several key evaluation questions and methods:\n- **Is this the correct action?** Checks if the agent chose the right step (e.g., using a search tool).\n- **Exact match comparison:** Compares the agent\u2019s answer to the ground truth to see if they match exactly.\n- **Does this answer use the context?** Ensures the agent\u2019s answer is based on the information retrieved.\n- **Extract numbers from each and compare:** Focuses on comparing specific data points, like the population number.\n- **Is this the right search result?** Evaluates if the search tool returned the correct information.\n- **Tag the right answer and do an exact match comparison with the retrieval:** Instructs to label the correct answer and directly compare it to the retrieved data.\n\nOverall, this approach helps developers ensure that each part of their application is functioning as expected by isolating and testing individual components against known correct answers.", "Technical patterns  \nSubjective evaluations\n\nThis content discusses how to create effective evaluation systems for automated testing, especially when using AI models like GPT-4. The main idea is that building a good scorecard for testing benefits from several rounds of detailed human review. This helps identify what is truly valuable in the responses generated by the AI.\n\nA key recommendation is to use a \"show rather than tell\" approach. This means providing concrete examples of what different scores look like (for example, what a score of 1, 3, or 8 out of 10 means). This helps the AI model understand the range and spread of possible evaluations.\n\nAn example scorecard is provided to illustrate how subjective evaluations can be structured:\n\n- The evaluator acts as an assistant who grades how well the AI has answered a customer's query.\n- Each submission is assessed using three metrics:\n  - **Relevance**: This measures how closely the answer matches the user's question, graded from 1 (not relevant at all) to 5 (highly relevant).\n  - **Credibility**: This assesses how trustworthy the sources are, graded from 1 (unreferenced) to 5 (established sources like government agencies or major newspapers).\n  - **Result**: This checks if the answer is correct based only on the information returned from the search and the user's question. The result is either \"correct\" or \"incorrect\".\n\nThe output is formatted as a JSON document, for example:  \n`{relevance: integer, credibility: integer, result: string}`\n\nA sample evaluation is given:\n- User asks: \"What is the population of Canada?\"\n- Assistant responds: \"Canada's population was estimated at 39,858,480 on April 1, 2023 by Statistics Canada.\"\n- The evaluation would be: `{relevance: 5, credibility: 5, result: correct}`\n\nThis approach helps ensure that evaluations are consistent, transparent, and useful for improving the AI's performance.", "Example framework\n\nThis framework explains how to systematically evaluate the effectiveness of a system, such as an AI model, by grouping evaluations into test suites called \"runs.\" These runs are executed in batches, and each run's results are logged in detail\u2014a process called \"tracing.\" Tracing allows you to investigate why failures occur, make improvements, and then rerun the evaluations to measure progress.\n\nA table is provided to illustrate how this process works in practice:\n\n- **Run ID**: Each evaluation batch is assigned a unique identifier.\n- **Model**: The specific version of the AI model being tested (e.g., gpt-3.5-turbo, gpt-4).\n- **Score**: The performance score out of 50 for each run.\n- **Annotation feedback**: Detailed feedback on errors, distinguishing between cases where the search results were correct but the answer was wrong, and cases where the search itself was incorrect.\n- **Changes since last run**: Describes what was changed or improved between runs to address previous issues.\n\nHere\u2019s a breakdown of the runs:\n\n1. The first run used the gpt-3.5-turbo model and scored 28 out of 50. There were 18 cases where the answer was incorrect despite correct search results, and 4 cases where the search itself was incorrect. No changes had been made prior to this run.\n2. The second run switched to the gpt-4 model, improving the score to 36 out of 50. There were 10 incorrect answers with correct search results and 4 incorrect searches. The main change was updating the model to GPT-4.\n3. The third run reverted to gpt-3.5-turbo, scoring 34 out of 50. There were 12 incorrect answers with correct search results and 4 incorrect searches. The change here was the addition of few-shot examples, which are sample inputs and outputs provided to help the model learn the task better.\n4. The fourth run, still using gpt-3.5-turbo, scored 42 out of 50. There were 8 incorrect answers with correct search results. Improvements included adding metadata to the search and applying prompt engineering to the answer step, which means refining how questions are asked to the model.\n5. The fifth run, again with gpt-3.5-turbo, achieved the highest score of 48 out of 50, with only 2 incorrect answers with correct search results. The main change was further prompt engineering for the answer step.\n\nThis example demonstrates how systematic evaluation, detailed feedback, and iterative improvements can significantly enhance the performance of a system. By carefully tracking changes and analyzing errors, you can make targeted adjustments that lead to measurable progress.", "Example framework\n\nThis framework illustrates how a user request is processed through an AI-powered system to handle a product return, using a T-shirt bought on Amazon as an example.\n\nThe process begins with the user stating, \"I want to return a T-shirt I bought on Amazon on March 3rd.\" This request is first handled by a component called the Router, which uses a large language model (LLM) to interpret the user's intent. The Router determines that the expected and predicted action is \"return,\" and this step is marked as a PASS, meaning the system correctly understood the user's request.\n\nNext, the request is passed to the Return Assistant, another LLM, which interacts with a knowledge base to check the return policy. The system checks if the request matches the expected policy (\"return_policy\") and confirms that it does, again marking this as a PASS.\n\nThe Return Assistant then generates a response: \"Sure - because we're within 14 days of the purchase, I can process the return.\" This response is evaluated both by the system and subjectively. The subjective evaluation checks if the response adheres to guidelines, scoring it on politeness (5), coherence (4), and relevancy (4), all of which pass the criteria.\n\nAdditionally, the system checks the ground truth for the scenario: the T-shirt is eligible for return, so the final outcome is a PASS.\n\nThe diagram uses color-coded dashed lines to indicate different types of evaluations:\n- Red dashed lines represent component evaluations, which are objective checks at each step.\n- Orange dashed lines represent subjective evaluations, which assess the quality and appropriateness of the response.\n\nIn summary, this framework demonstrates a step-by-step process for handling a user request using AI, ensuring both the technical accuracy and the quality of the response through multiple layers of evaluation.", "Best practices\n\nThis content outlines several important best practices for building and evaluating systems, especially in the context of developing applications that require testing and optimization.\n\n1. **Log everything**\n   - It is crucial to keep detailed records of all activities and test cases as you develop your application. By logging everything, you can later review these logs to find useful examples for evaluation (eval) cases. This helps in systematically testing and improving your system.\n\n2. **Create a feedback loop**\n   - Integrate evaluation processes directly into your application. This allows you to quickly run tests, make changes, and rerun tests to observe the effects of your modifications. \n   - Evaluations also serve as a valuable resource for creating structured examples, which can be used for advanced techniques like few-shot learning or fine-tuning your model during optimization.\n\n3. **Employ expert labelers who know the process**\n   - Involve experts who are familiar with the subject matter to help create your evaluation cases. Their expertise ensures that the test cases are realistic and closely resemble real-world scenarios, making your evaluations more meaningful and accurate.\n\n4. **Evaluate early and often**\n   - Start building evaluation processes as soon as you have a basic, functioning version of your application. Early evaluation provides a baseline, which is essential for making improvements. Without this baseline, it\u2019s difficult to measure progress or optimize effectively.\n   - Conducting evaluations early also helps you understand what constitutes a good response or outcome, guiding your development in the right direction.\n\nIn summary, these best practices emphasize the importance of thorough documentation, continuous feedback, expert involvement, and early, frequent evaluation to ensure the development of robust and effective systems."]}, {"filename": "fine-tuning-deck.pdf", "text": "Fine-tuning\nTechnique\n\nFebruary 2024\n\n\fOverview\n\nFine-tuning involves adjusting the \nparameters of pre-trained models on a \nspeci\ufb01c dataset or task. This process \nenhances the model's ability to generate \nmore accurate and relevant responses for \nthe given context by adapting it to the \nnuances and speci\ufb01c requirements of the \ntask at hand.\n\nExample use cases\n\n- Generate output in a consistent \n\n-\n\nformat\nProcess input by following speci\ufb01c \ninstructions\n\nWhat we\u2019ll cover\n\n\u25cf When to \ufb01ne-tune\n\n\u25cf Preparing the dataset\n\n\u25cf Best practices\n\n\u25cf Hyperparameters\n\n\u25cf Fine-tuning advances\n\n\u25cf Resources\n\n3\n\n\fWhat is Fine-tuning\n\nPublic Model\n\nTraining data\n\nTraining\n\nFine-tuned \nmodel\n\nFine-tuning a model consists of training the \nmodel to follow a set of given input/output \nexamples.\n\nThis will teach the model to behave in a \ncertain way when confronted with a similar \ninput in the future.\n\nWe recommend using 50-100 examples \n\neven if the minimum is 10.\n\n4\n\n\fWhen to \ufb01ne-tune\n\nGood for  \u2705\n\nNot good for  \u274c\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\nFollowing a given format or tone for the \n\noutput\n\nProcessing the input following speci\ufb01c, \n\ncomplex instructions\n\nImproving latency\n\nReducing token usage\n\n\u25cf\n\n\u25cf\n\n\u25cf\n\nTeaching the model new knowledge\n\u2794 Use RAG or custom models instead\n\nPerforming well at multiple, unrelated tasks\n\u2794 Do prompt-engineering or create multiple \n\nFT models instead\n\nInclude up-to-date content in responses\n\u2794 Use RAG instead\n\n5\n\n\fPreparing the dataset\n\nExample format\n\n{\n\n\"messages\": [\n\n{\n\n\"role\": \"system\",\n\"content\": \"Marv is a factual chatbot \nthat is also sarcastic.\"\n\n},\n{\n\n\"role\": \"user\",\n\"content\": \"What's the capital of \nFrance?\"\n\n},\n{\n\n\"role\": \"assistant\",\n\"content\": \"Paris, as if everyone \ndoesn't know that already.\"\n\n}\n\n]\n\n}\n\n.jsonl\n\n\u2794 Take the set of instructions and prompts that you \n\nfound worked best for the model prior to \ufb01ne-tuning. \nInclude them in every training example\n\n\u2794 If you would like to shorten the instructions or \n\nprompts, it may take more training examples to arrive \nat good results\n\nWe recommend using 50-100 examples \n\neven if the minimum is 10.\n\n6\n\n\fBest practices\n\nCurate examples carefully\n\nDatasets can be di\ufb03cult to build, start \nsmall and invest intentionally. \nOptimize for fewer high-quality \ntraining examples.\n\n\u25cf Consider \u201cprompt baking\u201d, or using a basic \nprompt to generate your initial examples\n\u25cf If your conversations are multi-turn, ensure \n\nyour examples are representative\n\n\u25cf Collect examples to target issues detected \n\nin evaluation\n\n\u25cf Consider the balance & diversity of data\n\u25cf Make sure your examples contain all the \n\ninformation needed in the response\n\nIterate on hyperparameters\n\nEstablish a baseline\n\nStart with the defaults and adjust \nbased on performance.\n\n\u25cf If the model does not appear to converge, \n\nincrease the learning rate multiplier\n\u25cf If the model does not follow the training \ndata as much as expected increase the \nnumber of epochs\n\n\u25cf If the model becomes less diverse than \n\nexpected decrease the # of epochs by 1-2\n\nAutomate your feedback \npipeline\n\nIntroduce automated evaluations to \nhighlight potential problem cases to \nclean up and use as training data.\n\nConsider the G-Eval approach of \nusing GPT-4 to perform automated \ntesting using a scorecard.\n\nOften users start with a \nzero-shot or few-shot prompt to \nbuild a baseline evaluation \nbefore graduating to \ufb01ne-tuning.\n\nOften users start with a \nzero-shot or few-shot prompt to \nbuild a baseline evaluation \nOptimize for latency and \nbefore graduating to \ufb01ne-tuning.\ntoken e\ufb03ciency\n\nWhen using GPT-4, once you \nhave a baseline evaluation and \ntraining examples consider \n\ufb01ne-tuning 3.5 to get similar \nperformance for less cost and \nlatency.\n\nExperiment with reducing or \nremoving system instructions \nwith subsequent \ufb01ne-tuned \nmodel versions.\n\n\fHyperparameters\n\nEpochs\nRefers to 1 full cycle through the training dataset\nIf you have hundreds of thousands of examples, we would recommend \nexperimenting with two epochs (or one) to avoid over\ufb01tting.\n\ndefault: auto (standard is 4)\n\nBatch size\nNumber of training examples used to train a single \nforward & backward pass\nIn general, we've found that larger batch sizes tend to work better for larger datasets\n\ndefault: ~0.2% x N* (max 256)\n\n*N = number of training examples\n\nLearning rate multiplier\nScaling factor for the original learning rate\nWe recommend experimenting with values between 0.02-0.2. We've found that \nlarger learning rates often perform better with larger batch sizes.\n\ndefault: 0.05, 0.1 or 0.2*\n\n*depends on \ufb01nal batch size\n\n8\n\n\f", "pages_description": ["Overview\n\nFine-tuning is the process of adjusting the parameters of pre-trained models using a specific dataset or for a particular task. This process helps the model generate more accurate and relevant responses by adapting it to the unique context, nuances, and requirements of the task at hand. Essentially, fine-tuning customizes a general model to perform better on specialized tasks.\n\nSome example use cases for fine-tuning include:\n- Generating output in a consistent format, which is useful when you need standardized responses.\n- Processing input by following specific instructions, ensuring the model behaves according to particular guidelines or rules.\n\nThe topics that will be covered include:\n- When to fine-tune: Understanding the scenarios where fine-tuning is beneficial.\n- Preparing the dataset: Steps and considerations for getting your data ready for fine-tuning.\n- Best practices: Tips and strategies to achieve the best results.\n- Hyperparameters: Key settings that influence the fine-tuning process.\n- Fine-tuning advances: Recent developments and improvements in fine-tuning techniques.\n- Resources: Additional materials and references for further learning.\n\nThis overview sets the stage for a deeper exploration into how and when to fine-tune models, ensuring you understand both the practical steps and the underlying concepts.", "What is Fine-tuning\n\nFine-tuning is a process used to adapt a pre-existing machine learning model, often called a \"public model,\" to perform better on a specific task by training it with new data. This is done by providing the model with a set of input and output examples, known as training data.\n\nThe diagram illustrates this process:\n- It starts with a public model, which is a general-purpose model trained on a large dataset.\n- Training data, which consists of specific examples relevant to the new task, is introduced.\n- Both the public model and the training data are fed into a training process.\n- The result of this training is a fine-tuned model, which is now better suited to handle tasks similar to those in the training data.\n\nThe main idea is that by showing the model how to respond to certain inputs, it learns to behave in a desired way when it encounters similar situations in the future.\n\nA recommendation is provided: while the minimum number of examples needed for fine-tuning is 10, it is better to use between 50 and 100 examples for more effective results. This helps the model learn more accurately and generalize better to new, similar inputs.", "When to fine-tune\n\nThis content explains the situations where fine-tuning a machine learning model is beneficial and when it is not the best approach.\n\n**Good for:**\n- Fine-tuning is useful when you want the model to follow a specific format or tone in its output. For example, if you need responses to always be formal or to follow a certain template.\n- It is also good for making the model process input according to detailed, complex instructions. This means if you have very specific requirements for how the model should handle information, fine-tuning can help.\n- Fine-tuning can help improve latency, which means it can make the model respond faster.\n- It can also help reduce token usage. Tokens are pieces of text the model processes, so using fewer tokens can make the model more efficient and cost-effective.\n\n**Not good for:**\n- Fine-tuning is not suitable for teaching the model new knowledge. If you want the model to learn new facts or information, you should use Retrieval-Augmented Generation (RAG) or build custom models instead.\n- It is not effective for making the model perform well at many different, unrelated tasks. In these cases, it is better to use prompt engineering (designing better prompts) or create multiple fine-tuned models, each specialized for a specific task.\n- Fine-tuning is not the right choice if you need the model to include up-to-date content in its responses. For this, using RAG is recommended, as it allows the model to access the latest information.\n\nIn summary, fine-tuning is best for making a model follow specific instructions, formats, or tones, and for improving efficiency. It is not suitable for adding new knowledge, handling many unrelated tasks, or providing the most current information.", "Preparing the dataset\n\nThis content explains how to prepare a dataset for training a chatbot model, focusing on the structure and best practices for creating effective training examples.\n\nAn example format is provided using a structured data representation. The example shows a conversation with three parts:\n- The \"system\" message sets the behavior of the chatbot, stating: \"Marv is a factual chatbot that is also sarcastic.\"\n- The \"user\" message asks: \"What's the capital of France?\"\n- The \"assistant\" (the chatbot) responds: \"Paris, as if everyone doesn't know that already.\" This response demonstrates the sarcastic tone specified in the system message.\n\nKey recommendations for preparing your dataset:\n- Use the set of instructions and prompts that worked best for your model before fine-tuning. These should be included in every training example to ensure consistency.\n- If you want to use shorter instructions or prompts, you may need to provide more training examples to achieve good results, as the model will have less context to learn from each example.\n\nA practical tip is provided: although the minimum number of examples required is 10, it is recommended to use 50-100 examples for better results.\n\nIn summary, the content emphasizes the importance of clear, consistent formatting and sufficient training examples to effectively fine-tune a chatbot model, especially when aiming for specific behaviors like sarcasm.", "Best practices\n\nThis content outlines several key strategies for effectively building and fine-tuning machine learning models, particularly in the context of language models like GPT-4.\n\nCurate examples carefully  \nBuilding datasets can be challenging, so it\u2019s recommended to start small and focus on creating a few high-quality training examples. Some tips include:\n- Use \"prompt baking,\" which means starting with a basic prompt to generate your initial examples.\n- If your data involves conversations with multiple exchanges, make sure your examples reflect this structure.\n- Collect examples that specifically address issues found during evaluation.\n- Ensure your data is balanced and diverse.\n- Make sure each example contains all the information needed for a complete response.\n\nIterate on hyperparameters  \nBegin with default settings and adjust them based on how the model performs:\n- If the model isn\u2019t learning as expected, try increasing the learning rate multiplier.\n- If the model isn\u2019t matching the training data well, increase the number of training cycles (epochs).\n- If the model\u2019s responses become less varied, try reducing the number of epochs by one or two.\n\nEstablish a baseline  \nStart by evaluating your model with a simple prompt (zero-shot or few-shot) to set a baseline for performance. This helps you understand how much improvement fine-tuning brings.\n\nAutomate your feedback pipeline  \nSet up automated evaluations to quickly identify problem areas in your model\u2019s responses. These can then be used as new training data. One approach is to use another model (like GPT-4) to automatically test and score your model\u2019s outputs.\n\nOptimize for latency and token efficiency  \nAfter establishing a baseline and collecting training examples, consider fine-tuning a smaller model (like GPT-3.5) to achieve similar results with lower cost and faster responses. Also, try reducing or removing extra instructions in your prompts as you refine your model, which can further improve efficiency.\n\nThese best practices help ensure that your model training process is efficient, cost-effective, and results in high-quality outputs.", "Hyperparameters\n\nThis content explains three important hyperparameters used in training machine learning models: epochs, batch size, and learning rate multiplier. Each of these plays a key role in how a model learns from data.\n\n**Epochs**\n- An epoch refers to one complete cycle through the entire training dataset.\n- If you have a very large dataset (hundreds of thousands of examples), it is recommended to use fewer epochs (such as one or two) to avoid overfitting, which is when the model learns the training data too well and performs poorly on new data.\n- The default setting is usually automatic, with a standard value of 4 epochs.\n\n**Batch Size**\n- Batch size is the number of training examples used to train the model in a single forward and backward pass.\n- Larger batch sizes are generally better for larger datasets, as they can make training more efficient and stable.\n- The default batch size is about 0.2% of the total number of training examples, with a maximum of 256.\n- For example, if you have 10,000 training examples, the batch size would be around 20.\n\n**Learning Rate Multiplier**\n- The learning rate multiplier is a scaling factor applied to the original learning rate, which controls how much the model's weights are updated during training.\n- It is recommended to experiment with values between 0.02 and 0.2.\n- Larger learning rates often work better with larger batch sizes.\n- The default values are 0.05, 0.1, or 0.2, depending on the final batch size.\n\nIn summary, these hyperparameters\u2014epochs, batch size, and learning rate multiplier\u2014are crucial for controlling the training process of a machine learning model. Adjusting them appropriately can help achieve better performance and avoid common issues like overfitting."]}]