{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec5f67a9",
   "metadata": {},
   "source": [
    "# Parsing PDF documents for RAG applications\n",
    "\n",
    "This notebook shows how to leverage GPT-41. to turn rich PDF documents such as slide decks or exports from web pages into usable content for your RAG application.\n",
    "\n",
    "This technique can be used if you have a lot of unstructured data containing valuable information that you want to be able to retrieve as part of your RAG pipeline.\n",
    "\n",
    "For example, you could build a Knowledge Assistant that could answer user queries about your company or product based on information contained in PDF documents. \n",
    "\n",
    "The example documents used in this notebook are located at [data/example_pdfs](data/example_pdfs). They are related to OpenAI's APIs and various techniques that can be used as part of LLM projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8f6820",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "In this section, we will process our input data to prepare it for retrieval.\n",
    "\n",
    "We will do this in 2 ways:\n",
    "\n",
    "1. Extracting text with pdfminer\n",
    "2. Converting the PDF pages to images to analyze them with GPT-4o\n",
    "\n",
    "You can skip the 1st method if you want to only use the content inferred from the image analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74eb2df8",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "We need to install a few libraries to convert the PDF to images and extract the text (optional).\n",
    "\n",
    "**Note: You need to install `poppler` on your machine for the `pdf2image` library to work. You can follow the instructions to install it [here](https://pypi.org/project/pdf2image/).**. \n",
    "\n",
    "If you have brew you can use `brew install poppler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f08b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pdf2image import convert_from_path\n",
    "from pdf2image.exceptions import (\n",
    "    PDFInfoNotInstalledError,\n",
    "    PDFPageCountError,\n",
    "    PDFSyntaxError\n",
    ")\n",
    "from pdfminer.high_level import extract_text\n",
    "import base64\n",
    "import io\n",
    "import os\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "import re\n",
    "import pandas as pd \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "import numpy as np\n",
    "from rich import print\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f713d87",
   "metadata": {},
   "source": [
    "### File processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6696a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_doc_to_images(path):\n",
    "    images = convert_from_path(path)\n",
    "    return images\n",
    "\n",
    "def extract_text_from_doc(path):\n",
    "    text = extract_text(path)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41992d04",
   "metadata": {},
   "source": [
    "#### Testing with an example\n",
    "\n",
    "If you get this error:\n",
    "`PDFInfoNotInstalledError: Unable to get page count. Is poppler installed and in PATH?`\n",
    "\n",
    "You need to install poppler:\n",
    "`brew install poppler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ab392d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"example_pdfs/fine-tuning-deck.pdf\"\n",
    "\n",
    "images = convert_doc_to_images(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2575a284",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = extract_text_from_doc(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2c432a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in images:\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be69a779",
   "metadata": {},
   "source": [
    "### Image analysis with GPT-4.1\n",
    "\n",
    "After converting a PDF file to multiple images, we'll use GPT-4.1 to analyze the content based on the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edcbd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing OpenAI client - see https://platform.openai.com/docs/quickstart?context=python\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb376547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting images to base64 encoded images in a data URI format to use with the ChatCompletions API\n",
    "def get_img_uri(img):\n",
    "    png_buffer = io.BytesIO()\n",
    "    img.save(png_buffer, format=\"PNG\")\n",
    "    png_buffer.seek(0)\n",
    "\n",
    "    base64_png = base64.b64encode(png_buffer.read()).decode('utf-8')\n",
    "\n",
    "    data_uri = f\"data:image/png;base64,{base64_png}\"\n",
    "    return data_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a606d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = '''\n",
    "You will be provided with an image of a PDF page or a slide. Your goal is to deliver a detailed and engaging presentation about the content you see, using clear and accessible language suitable for a 101-level audience.\n",
    "\n",
    "If there is an identifiable title, start by stating the title to provide context for your audience.\n",
    "\n",
    "Describe visual elements in detail:\n",
    "\n",
    "- **Diagrams**: Explain each component and how they interact. For example, \"The process begins with X, which then leads to Y and results in Z.\"\n",
    "  \n",
    "- **Tables**: Break down the information logically. For instance, \"Product A costs X dollars, while Product B is priced at Y dollars.\"\n",
    "\n",
    "Focus on the content itself rather than the format:\n",
    "\n",
    "- **DO NOT** include terms referring to the content format.\n",
    "  \n",
    "- **DO NOT** mention the content type. Instead, directly discuss the information presented.\n",
    "\n",
    "Keep your explanation comprehensive yet concise:\n",
    "\n",
    "- Be exhaustive in describing the content, as your audience cannot see the image.\n",
    "  \n",
    "- Exclude irrelevant details such as page numbers or the position of elements on the image.\n",
    "\n",
    "Use clear and accessible language:\n",
    "\n",
    "- Explain technical terms or concepts in simple language appropriate for a 101-level audience.\n",
    "\n",
    "Engage with the content:\n",
    "\n",
    "- Interpret and analyze the information where appropriate, offering insights to help the audience understand its significance.\n",
    "\n",
    "------\n",
    "\n",
    "If there is an identifiable title, present the output in the following format:\n",
    "\n",
    "{TITLE}\n",
    "\n",
    "{Content description}\n",
    "\n",
    "If there is no clear title, simply provide the content description.\n",
    "'''\n",
    "\n",
    "def analyze_image(data_uri):\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-4.1\",\n",
    "        input=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                    \"type\": \"input_image\",\n",
    "                    \"image_url\": f\"{data_uri}\"\n",
    "                    }\n",
    "                ]\n",
    "                },\n",
    "        ],\n",
    "        temperature=0,\n",
    "        top_p=0.1\n",
    "    )\n",
    "    return response.output[0].content[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff47a90b",
   "metadata": {},
   "source": [
    "#### Testing with an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea9dc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = images[0]\n",
    "display(img)\n",
    "data_uri = get_img_uri(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a272d1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = analyze_image(data_uri)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b8efaf",
   "metadata": {},
   "source": [
    "#### Processing all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d3f8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "files_path = \"example_pdfs\"\n",
    "\n",
    "all_items = os.listdir(files_path)\n",
    "files = [item for item in all_items if os.path.isfile(os.path.join(files_path, item))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d1021a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_doc_image(img):\n",
    "    img_uri = get_img_uri(img)\n",
    "    data = analyze_image(img_uri)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4170b84f",
   "metadata": {},
   "source": [
    "We will list all files in the example folder and process them by \n",
    "1. Extracting the text\n",
    "2. Converting the docs to images\n",
    "3. Analyzing pages with GPT-4.1\n",
    "\n",
    "Note: This takes about ~2 mins to run. Feel free to skip and load directly the result file (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dace007b",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "\n",
    "for f in files:\n",
    "    \n",
    "    path = f\"{files_path}/{f}\"\n",
    "    doc = {\n",
    "        \"filename\": f\n",
    "    }\n",
    "    text = extract_text_from_doc(path)\n",
    "    doc['text'] = text\n",
    "    imgs = convert_doc_to_images(path)\n",
    "    pages_description = []\n",
    "    \n",
    "    print(f\"Analyzing pages for doc {f}\")\n",
    "    \n",
    "    # Concurrent execution\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        \n",
    "        # Removing 1st slide as it's usually just an intro\n",
    "        futures = [\n",
    "            executor.submit(analyze_doc_image, img)\n",
    "            for img in imgs[1:]\n",
    "        ]\n",
    "        \n",
    "        with tqdm(total=len(imgs)-1) as pbar:\n",
    "            for _ in concurrent.futures.as_completed(futures):\n",
    "                pbar.update(1)\n",
    "        \n",
    "        for f in futures:\n",
    "            res = f.result()\n",
    "            pages_description.append(res)\n",
    "        \n",
    "    doc['pages_description'] = pages_description\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b733ba0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving result to file for later\n",
    "json_path = \"parsed_pdf_docs.json\"\n",
    "\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(docs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535770e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: load content from the saved file\n",
    "with open(json_path, 'r') as f:\n",
    "    docs = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e507ee4e",
   "metadata": {},
   "source": [
    "### Embedding content\n",
    "Before embedding the content, we will chunk it logically by page.\n",
    "For real-world scenarios, you could explore more advanced ways to chunk the content:\n",
    "- Cutting it into smaller pieces\n",
    "- Adding data - such as the slide title, deck title and/or the doc description - at the beginning of each piece of content. That way, each independent chunk can be in context\n",
    "\n",
    "For the sake of brevity, we will use a very simple chunking strategy and rely on separators to split the text by page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ffb8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking content by page and merging together slides text & description if applicable\n",
    "content = []\n",
    "for doc in docs:\n",
    "    # Removing first slide as well\n",
    "    text = doc['text'].split('\\f')[1:]\n",
    "    description = doc['pages_description']\n",
    "    description_indexes = []\n",
    "    for i in range(len(text)):\n",
    "        slide_content = text[i] + '\\n'\n",
    "        # Trying to find matching slide description\n",
    "        slide_title = text[i].split('\\n')[0]\n",
    "        for j in range(len(description)):\n",
    "            description_title = description[j].split('\\n')[0]\n",
    "            if slide_title.lower() == description_title.lower():\n",
    "                slide_content += description[j].replace(description_title, '')\n",
    "                # Keeping track of the descriptions added\n",
    "                description_indexes.append(j)\n",
    "        # Adding the slide content + matching slide description to the content pieces\n",
    "        content.append(slide_content) \n",
    "    # Adding the slides descriptions that weren't used\n",
    "    for j in range(len(description)):\n",
    "        if j not in description_indexes:\n",
    "            content.append(description[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f972358",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in content:\n",
    "    print(c)\n",
    "    print(\"\\n\\n-------------------------------\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b84f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up content\n",
    "# Removing trailing spaces, additional line breaks, page numbers and references to the content being a slide\n",
    "clean_content = []\n",
    "for c in content:\n",
    "    text = c.replace(' \\n', '').replace('\\n\\n', '\\n').replace('\\n\\n\\n', '\\n').strip()\n",
    "    text = re.sub(r\"(?<=\\n)\\d{1,2}\", \"\", text)\n",
    "    text = re.sub(r\"\\b(?:the|this)\\s*slide\\s*\\w+\\b\", \"\", text, flags=re.IGNORECASE)\n",
    "    clean_content.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a13c8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in clean_content:\n",
    "    print(c)\n",
    "    print(\"\\n\\n-------------------------------\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c183f248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the embeddings\n",
    "# We'll save to a csv file here for testing purposes but this is where you should load content in your vectorDB.\n",
    "df = pd.DataFrame(clean_content, columns=['content'])\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e498ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model = \"text-embedding-3-large\"\n",
    "\n",
    "def get_embeddings(text):\n",
    "    embeddings = client.embeddings.create(\n",
    "      model=\"text-embedding-3-small\",\n",
    "      input=text,\n",
    "      encoding_format=\"float\"\n",
    "    )\n",
    "    return embeddings.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55ffea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['embeddings'] = df['content'].apply(lambda x: get_embeddings(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed508fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving locally for later\n",
    "data_path = \"parsed_pdf_docs_with_embeddings.csv\"\n",
    "df.to_csv(data_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d46009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: load data from saved file\n",
    "df = pd.read_csv(data_path)\n",
    "df[\"embeddings\"] = df.embeddings.apply(literal_eval).apply(np.array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f28788",
   "metadata": {},
   "source": [
    "## Retrieval-augmented generation\n",
    "\n",
    "The last step of the process is to generate outputs in response to input queries, after retrieving content as context to reply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7edc01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = '''\n",
    "    You will be provided with an input prompt and content as context that can be used to reply to the prompt.\n",
    "    \n",
    "    You will do 2 things:\n",
    "    \n",
    "    1. First, you will internally assess whether the content provided is relevant to reply to the input prompt. \n",
    "    \n",
    "    2a. If that is the case, answer directly using this content. If the content is relevant, use elements found in the content to craft a reply to the input prompt.\n",
    "\n",
    "    2b. If the content is not relevant, use your own knowledge to reply or say that you don't know how to respond if your knowledge is not sufficient to answer.\n",
    "    \n",
    "    Stay concise with your answer, replying specifically to the input prompt without mentioning additional information provided in the context content.\n",
    "'''\n",
    "\n",
    "model=\"gpt-4.1\"\n",
    "\n",
    "def search_content(df, input_text, top_k):\n",
    "    embedded_value = get_embeddings(input_text)\n",
    "    df[\"similarity\"] = df.embeddings.apply(lambda x: cosine_similarity(np.array(x).reshape(1,-1), np.array(embedded_value).reshape(1, -1)))\n",
    "    res = df.sort_values('similarity', ascending=False).head(top_k)\n",
    "    return res\n",
    "\n",
    "def get_similarity(row):\n",
    "    similarity_score = row['similarity']\n",
    "    if isinstance(similarity_score, np.ndarray):\n",
    "        similarity_score = similarity_score[0][0]\n",
    "    return similarity_score\n",
    "\n",
    "def generate_output(input_prompt, similar_content, threshold = 0.5):\n",
    "    \n",
    "    content = similar_content.iloc[0]['content']\n",
    "    \n",
    "    # Adding more matching content if the similarity is above threshold\n",
    "    if len(similar_content) > 1:\n",
    "        for i, row in similar_content.iterrows():\n",
    "            similarity_score = get_similarity(row)\n",
    "            if similarity_score > threshold:\n",
    "                content += f\"\\n\\n{row['content']}\"\n",
    "            \n",
    "    prompt = f\"INPUT PROMPT:\\n{input_prompt}\\n-------\\nCONTENT:\\n{content}\"\n",
    "    \n",
    "    completion = client.responses.create(\n",
    "        model=model,\n",
    "        temperature=0.5,\n",
    "        input=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return completion.output[0].content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f9fb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example user queries related to the content\n",
    "example_inputs = [\n",
    "    'What are the main models you offer?',\n",
    "    'Do you have a speech recognition model?',\n",
    "    'Which embedding model should I use for non-English use cases?',\n",
    "    'Can I introduce new knowledge in my LLM app using RAG?',\n",
    "    'How many examples do I need to fine-tune a model?',\n",
    "    'Which metric can I use to evaluate a summarization task?',\n",
    "    'Give me a detailed example for an evaluation process where we are looking for a clear answer to compare to a ground truth.',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313d2f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the RAG pipeline on each example\n",
    "for ex in example_inputs:\n",
    "    print(f\"[deep_pink4][bold]QUERY:[/bold] {ex}[/deep_pink4]\\n\\n\")\n",
    "    matching_content = search_content(df, ex, 3)\n",
    "    print(f\"[grey37][b]Matching content:[/b][/grey37]\\n\")\n",
    "    for i, match in matching_content.iterrows():\n",
    "        print(f\"[grey37][i]Similarity: {get_similarity(match):.2f}[/i][/grey37]\")\n",
    "        print(f\"[grey37]{match['content'][:100]}{'...' if len(match['content']) > 100 else ''}[/[grey37]]\\n\\n\")\n",
    "    reply = generate_output(ex, matching_content)\n",
    "    print(f\"[turquoise4][b]REPLY:[/b][/turquoise4]\\n\\n[spring_green4]{reply}[/spring_green4]\\n\\n--------------\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0d14e4",
   "metadata": {},
   "source": [
    "## Wrapping up\n",
    "\n",
    "In this notebook, we have learned how to develop a basic RAG pipeline based on PDF documents. This includes:\n",
    "\n",
    "- How to parse pdf documents, taking slide decks and an export from an HTML page as examples, using a python library as well as GPT-4o to interpret the visuals\n",
    "- How to process the extracted content, clean it and chunk it into several pieces\n",
    "- How to embed the processed content using OpenAI embeddings\n",
    "- How to retrieve content that is relevant to an input query\n",
    "- How to use GPT-4o to generate an answer using the retrieved content as context\n",
    "\n",
    "If you want to explore further, consider these optimisations:\n",
    "\n",
    "- Playing around with the prompts provided as examples\n",
    "- Chunking the content further and adding metadata as context to each chunk\n",
    "- Adding rule-based filtering on the retrieval results or re-ranking results to surface to most relevant content\n",
    "\n",
    "You can apply the techniques covered in this notebook to multiple use cases, such as assistants that can access your proprietary data, customer service or FAQ bots that can read from your internal policies, or anything that requires leveraging rich documents that would be better understood as images."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
